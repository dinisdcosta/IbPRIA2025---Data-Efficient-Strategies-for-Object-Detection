{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e01772",
   "metadata": {},
   "source": [
    "# 📘 IbPRIA 2025 - Data-Efficient Strategies for Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c69f66",
   "metadata": {},
   "source": [
    "> 📌 **Note**: You can run this notebook on:\n",
    ">\n",
    "> - 💻 Your local machine (Python ≥ 3.8, see `requirements.txt`)\n",
    "> - 🌐 [Google Colab](https://colab.research.google.com/github/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/blob/main/hands_on_notebook.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec05742",
   "metadata": {},
   "source": [
    "## ⚙️ Preparation to Run This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16179b25",
   "metadata": {},
   "source": [
    "To ensure faster training and better performance, it is recommended to use a **GPU** when running this notebook.\n",
    "\n",
    "If you're using **Google Colab**, make sure the runtime is configured to use a **GPU** or **TPU**.\n",
    "\n",
    "> 📌 You can change the runtime by going to:  \n",
    "> `Runtime` → `Change runtime type` → Select **GPU** or **TPU**\n",
    "\n",
    "⬇️ See the image below for guidance.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** You can also run this notebook **locally** using your own machine with **CPU only**.  \n",
    "Training may be slower, but all code will still work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a228b69",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/colab_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b17048",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/colab_2_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "861d6a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10325.71s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (8.3.160)\n",
      "Requirement already satisfied: labelimg in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.8.6)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.13.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (3.9.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (11.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (2.7.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 1)) (2.0.14)\n",
      "Requirement already satisfied: pyqt5 in ./.venv/lib/python3.9/site-packages (from labelimg->-r requirements.txt (line 2)) (5.15.11)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.9/site-packages (from labelimg->-r requirements.txt (line 2)) (5.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 4)) (6.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 4)) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics->-r requirements.txt (line 1)) (2025.6.15)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 1)) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 1)) (2025.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.8.0->ultralytics->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.15 in ./.venv/lib/python3.9/site-packages (from pyqt5->labelimg->-r requirements.txt (line 2)) (12.17.0)\n",
      "Requirement already satisfied: PyQt5-Qt5<5.16.0,>=5.15.2 in ./.venv/lib/python3.9/site-packages (from pyqt5->labelimg->-r requirements.txt (line 2)) (5.15.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if \"COLAB_RELEASE_TAG\" in os.environ:\n",
    "    print(\"✅ Running on Google Colab\")\n",
    "    # Clone the repository if running in Google Colab\n",
    "    !git clone https://github.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection.git\n",
    "    # Change working directory\n",
    "    os.chdir(\"IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection\")\n",
    "\n",
    "%pip install -r requirements.txt # Install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6e2c1",
   "metadata": {},
   "source": [
    "## ✏️ Annotation Guidelines for Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35bf29d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In object detection tasks, each image must be labeled with **bounding boxes** that enclose every object the model is expected to detect. For each bounding box, a corresponding **class label** must be assigned.\n",
    "\n",
    "There are several tools available for this annotation process:\n",
    "\n",
    "- If you're working with a **small dataset** and need a lightweight solution, we recommend [labelImg](https://pypi.org/project/labelImg/).\n",
    "- For **larger projects** with multiple annotators, we suggest using a more robust platform such as [Label Studio](https://labelstud.io/), which provide better support for collaboration.\n",
    "\n",
    "### Bounding Box Best Practices\n",
    "\n",
    "A bounding box is defined by its **center coordinates**, **width**, and **height**. For accurate annotations:\n",
    "\n",
    "- The **center of the bounding box** should align with the center of the corresponding object.\n",
    "- The edges of the bounding box should **not exceed the boundaries** of the object.\n",
    "- Boxes should be **tight** but not overly precise — enough to capture the object clearly without including background noise.\n",
    "\n",
    "> Ensuring consistent and accurate annotation is crucial, as it directly impacts the performance and reliability of the trained object detection model.\n",
    "\n",
    "### Running LabelImg Locally\n",
    "\n",
    "If you're running this notebook or working locally (not on Colab), you can install and launch **LabelImg** using the commands below:\n",
    "\n",
    "```bash\n",
    "pip install labelImg\n",
    "```\n",
    "**Important:** LabelImg works best with Python 3.9 or lower. Newer versions may cause compatibility issues. We recommend using a virtual environment with Python 3.9 if needed.\n",
    "\n",
    "To lauch the tool:\n",
    "```bash\n",
    "labelImg\n",
    "```\n",
    "\n",
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/labelimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7c5bb",
   "metadata": {},
   "source": [
    "## 📂 Dataset Format for YOLO Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825ff24",
   "metadata": {},
   "source": [
    "\n",
    "To train a YOLO model, your dataset must follow a specific structure and include a `.yaml` configuration file that defines where your images and annotations are located, along with the list of classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Folder Structure\n",
    "\n",
    "Each image should have a corresponding `.txt` annotation file with the same name (e.g., `image1.jpg` ↔ `image1.txt`), and these should be placed in the appropriate subfolders.\n",
    "\n",
    "```text\n",
    "dataset/\n",
    "├── train/\n",
    "├── val/\n",
    "├── test/\n",
    "└── data.yaml\n",
    "```\n",
    "\n",
    "> **Note:** YOLO expects labels in `.txt` format where each line corresponds to one bounding box, with the format:\n",
    ">\n",
    "> ```\n",
    "> <class_id> <x_center> <y_center> <width> <height>\n",
    "> ```\n",
    "> All values must be normalized (from 0 to 1) relative to image size.\n",
    "\n",
    "---\n",
    "\n",
    "### `data.yaml` Example\n",
    "\n",
    "```yaml\n",
    "train: dataset/train/images\n",
    "val: dataset/val/images\n",
    "test: dataset/test/images\n",
    "\n",
    "nc: 1  # number of classes\n",
    "names: ['WF'] #name of the class that MUST match the <class_id> in the annotation file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2cbe9",
   "metadata": {},
   "source": [
    "## 👁️ Object Detection with YOLOv11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd2334",
   "metadata": {},
   "source": [
    "\n",
    "Once your dataset is prepared and organized correctly, you can begin training your YOLOv11 model. In this example, we’ll use the **nano version** of YOLOv11 (`yolov11n.pt`) and train on a custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69141f9a",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Dataset Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32724b41",
   "metadata": {},
   "source": [
    "We use a utility function to split the dataset into **training** and **validation** sets, assuming a separate **test set** already exists.\n",
    "\n",
    "This example splits a dataset of 200 images into:\n",
    "- **60% for training**\n",
    "- **20% for validation**\n",
    "- The remaining **20% are reserved as the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e9ad0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and validation splits saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils import get_split, split_dataset\n",
    "\n",
    "# Get train/val split from dataset (test already fixed)\n",
    "train, val = get_split(train_size=0.6, val_size=0.2, dataset_size=200)\n",
    "\n",
    "# Save to dataset/run/train and dataset/run/val\n",
    "split_dataset(train_split=train, val_split=val)\n",
    "print(\"Train and validation splits saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19989df4",
   "metadata": {},
   "source": [
    "### Step 2: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8507b8",
   "metadata": {},
   "source": [
    "After the dataset splitting process, it is important to define the training parameters.\n",
    "\n",
    "Some important parameters to define are: **image size**, number of **epochs**, how to initialize the model **weights**, and the **batch size**.\n",
    "\n",
    "Let us clarify each of them:\n",
    "\n",
    "**Image size** refers to the resolution to which every input image is resized before being fed into the yolov11 model. This fixed-size input is necessary because the model architecture (fully convolutional with final dense layers) expects inputs of the same size.\n",
    "\n",
    "An **epoch** is one complete pass through the entire training dataset by the learning algorithm.\n",
    "\n",
    "Pre-trained **weights** are obtained by training a model on a large dataset before applying it to a specific task. This approach leverages **transfer learning**, allowing the model to take advantage of already learned general features such as edges, textures, or shapes. We will further explore **transfer learning** and its advantages.\n",
    "\n",
    "Finally, **batch size** is the number of training samples processed before the model updates its weights.\n",
    "\n",
    "For each batch:\n",
    "- The model makes predictions.\n",
    "- The loss is computed.\n",
    "- The optimizer adjusts weights using gradients from that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a6a8607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1280, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov11n_custom, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=example_simple_detector, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=example_simple_detector/yolov11n_custom, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 8755.7±1148.1 MB/s, size: 838.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train... 120 images, 0 backgrounds, 0 corrupt: 100%|██████████| 120/120 [00:00<00:00, 1298.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 7902.8±1068.0 MB/s, size: 748.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val... 40 images, 0 backgrounds, 0 corrupt: 100%|██████████| 40/40 [00:00<00:00, 2066.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to example_simple_detector/yolov11n_custom/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mexample_simple_detector/yolov11n_custom\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      2.166      3.361      1.042        221       1280: 100%|██████████| 30/30 [01:53<00:00,  3.78s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:12<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932     0.0113     0.0704    0.00987    0.00175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.779      1.992     0.9148        100       1280: 100%|██████████| 30/30 [01:49<00:00,  3.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:16<00:00,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932     0.0588      0.365      0.231     0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G       1.83       1.89     0.9192        101       1280: 100%|██████████| 30/30 [01:47<00:00,  3.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  20%|██        | 1/5 [00:04<00:18,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  40%|████      | 2/5 [00:09<00:14,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  60%|██████    | 3/5 [00:14<00:10,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  80%|████████  | 4/5 [00:19<00:04,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:24<00:00,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932      0.506      0.537      0.472      0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.858       1.78     0.9515        138       1280: 100%|██████████| 30/30 [01:44<00:00,  3.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  20%|██        | 1/5 [00:04<00:18,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  40%|████      | 2/5 [00:09<00:14,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  60%|██████    | 3/5 [00:14<00:10,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  80%|████████  | 4/5 [00:19<00:04,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:24<00:00,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932      0.639       0.46      0.503       0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.774      1.711     0.9285        232       1280: 100%|██████████| 30/30 [01:50<00:00,  3.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  20%|██        | 1/5 [00:04<00:18,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  40%|████      | 2/5 [00:09<00:13,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  80%|████████  | 4/5 [00:18<00:04,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:22<00:00,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932      0.694      0.633       0.69      0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.637      1.608     0.9065        123       1280: 100%|██████████| 30/30 [01:52<00:00,  3.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  20%|██        | 1/5 [00:04<00:17,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  40%|████      | 2/5 [00:09<00:13,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:22<00:00,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932      0.717      0.672      0.734      0.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.572      1.482     0.8979        228       1280: 100%|██████████| 30/30 [01:43<00:00,  3.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:19<00:00,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932      0.742      0.709      0.782       0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.534      1.416     0.8966        116       1280: 100%|██████████| 30/30 [01:42<00:00,  3.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:18<00:00,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932      0.768       0.73      0.805      0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.523      1.401     0.8807        214       1280: 100%|██████████| 30/30 [01:42<00:00,  3.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:17<00:00,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932      0.779      0.744      0.822      0.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.448      1.291     0.8887        618       1280: 100%|██████████| 30/30 [01:47<00:00,  3.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:16<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932      0.798      0.738      0.826      0.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.353 hours.\n",
      "Optimizer stripped from example_simple_detector/yolov11n_custom/weights/last.pt, 5.6MB\n",
      "Optimizer stripped from example_simple_detector/yolov11n_custom/weights/best.pt, 5.6MB\n",
      "\n",
      "Validating example_simple_detector/yolov11n_custom/weights/best.pt...\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:15<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       1932      0.797      0.739      0.826      0.504\n",
      "Speed: 3.8ms preprocess, 166.4ms inference, 0.0ms loss, 153.7ms postprocess per image\n",
      "Results saved to \u001b[1mexample_simple_detector/yolov11n_custom\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from utils import train_yolo\n",
    "\n",
    "project = \"example_simple_detector\" # Project directory for saving results\n",
    "\n",
    "name = \"yolov11n_custom\"\n",
    "\n",
    "image_size = 1280 # Image size for training\n",
    "epochs = 10 # Number of epochs for training\n",
    "batch_size = 4 # Batch size for training\n",
    "\n",
    "# Train the YOLO model\n",
    "trained = train_yolo(\n",
    "    img=image_size, # Image size\n",
    "    epochs=epochs, # Number of epochs\n",
    "    data=\"dataset/data.yaml\", # Path to data configuration file\n",
    "    weights=\"yolo11n.pt\", # Pre-trained weights from COCO dataset\n",
    "    batch=batch_size, # Batch size\n",
    "    name=name, # Name of the training run\n",
    "    project=project # Project directory for saving results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b4ab55",
   "metadata": {},
   "source": [
    "### Step 3: Detection with the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9221309",
   "metadata": {},
   "source": [
    "Once your model has been trained, you can use it to detect objects in new images.\n",
    "\n",
    "Below is an example of how to **load your trained YOLOv11 model**, run inference on a single image, and **visualize the results** using `OpenCV` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b017991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING ⚠️ \n",
      "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (frame 1/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 16 Whiteflys, 118.8ms\n",
      "video 1/1 (frame 2/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 15 Whiteflys, 120.0ms\n",
      "video 1/1 (frame 3/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 101.6ms\n",
      "video 1/1 (frame 4/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 16 Whiteflys, 93.7ms\n",
      "video 1/1 (frame 5/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 101.6ms\n",
      "video 1/1 (frame 6/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 104.1ms\n",
      "video 1/1 (frame 7/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 95.7ms\n",
      "video 1/1 (frame 8/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 106.9ms\n",
      "video 1/1 (frame 9/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 100.4ms\n",
      "video 1/1 (frame 10/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 93.9ms\n",
      "video 1/1 (frame 11/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 103.5ms\n",
      "video 1/1 (frame 12/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 98.6ms\n",
      "video 1/1 (frame 13/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 96.3ms\n",
      "video 1/1 (frame 14/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 99.6ms\n",
      "video 1/1 (frame 15/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 98.5ms\n",
      "video 1/1 (frame 16/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 98.3ms\n",
      "video 1/1 (frame 17/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 93.0ms\n",
      "video 1/1 (frame 18/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 92.5ms\n",
      "video 1/1 (frame 19/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 104.4ms\n",
      "video 1/1 (frame 20/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 95.8ms\n",
      "video 1/1 (frame 21/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 97.0ms\n",
      "video 1/1 (frame 22/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 94.6ms\n",
      "video 1/1 (frame 23/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 98.4ms\n",
      "video 1/1 (frame 24/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 93.7ms\n",
      "video 1/1 (frame 25/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 99.4ms\n",
      "video 1/1 (frame 26/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 102.4ms\n",
      "video 1/1 (frame 27/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 101.8ms\n",
      "video 1/1 (frame 28/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 96.6ms\n",
      "video 1/1 (frame 29/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 97.0ms\n",
      "video 1/1 (frame 30/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 98.2ms\n",
      "video 1/1 (frame 31/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 102.8ms\n",
      "video 1/1 (frame 32/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 98.3ms\n",
      "video 1/1 (frame 33/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 93.3ms\n",
      "video 1/1 (frame 34/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 105.0ms\n",
      "video 1/1 (frame 35/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 100.9ms\n",
      "video 1/1 (frame 36/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 92.8ms\n",
      "video 1/1 (frame 37/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 97.2ms\n",
      "video 1/1 (frame 38/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 95.2ms\n",
      "video 1/1 (frame 39/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 94.3ms\n",
      "video 1/1 (frame 40/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 95.2ms\n",
      "video 1/1 (frame 41/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 104.7ms\n",
      "video 1/1 (frame 42/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 107.1ms\n",
      "video 1/1 (frame 43/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 120.9ms\n",
      "video 1/1 (frame 44/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 95.4ms\n",
      "video 1/1 (frame 45/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 95.2ms\n",
      "video 1/1 (frame 46/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 101.2ms\n",
      "video 1/1 (frame 47/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 97.8ms\n",
      "video 1/1 (frame 48/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 99.8ms\n",
      "video 1/1 (frame 49/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 101.2ms\n",
      "video 1/1 (frame 50/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 114.0ms\n",
      "video 1/1 (frame 51/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 94.2ms\n",
      "video 1/1 (frame 52/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 101.9ms\n",
      "video 1/1 (frame 53/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 102.3ms\n",
      "video 1/1 (frame 54/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 96.4ms\n",
      "video 1/1 (frame 55/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 96.1ms\n",
      "video 1/1 (frame 56/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 96.4ms\n",
      "video 1/1 (frame 57/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 100.3ms\n",
      "video 1/1 (frame 58/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 93.7ms\n",
      "video 1/1 (frame 59/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 97.3ms\n",
      "video 1/1 (frame 60/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 95.9ms\n",
      "video 1/1 (frame 61/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 93.2ms\n",
      "video 1/1 (frame 62/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 101.2ms\n",
      "video 1/1 (frame 63/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 64/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 97.3ms\n",
      "video 1/1 (frame 65/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 93.7ms\n",
      "video 1/1 (frame 66/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 94.1ms\n",
      "video 1/1 (frame 67/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 94.4ms\n",
      "video 1/1 (frame 68/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 99.8ms\n",
      "video 1/1 (frame 69/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 94.1ms\n",
      "video 1/1 (frame 70/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 93.8ms\n",
      "video 1/1 (frame 71/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 95.4ms\n",
      "video 1/1 (frame 72/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 130.7ms\n",
      "video 1/1 (frame 73/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 99.5ms\n",
      "video 1/1 (frame 74/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 94.2ms\n",
      "video 1/1 (frame 75/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 94.4ms\n",
      "video 1/1 (frame 76/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 97.4ms\n",
      "video 1/1 (frame 77/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 99.2ms\n",
      "video 1/1 (frame 78/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 94.7ms\n",
      "video 1/1 (frame 79/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 103.8ms\n",
      "video 1/1 (frame 80/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 81/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 98.7ms\n",
      "video 1/1 (frame 82/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 96.0ms\n",
      "video 1/1 (frame 83/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 107.1ms\n",
      "video 1/1 (frame 84/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 102.1ms\n",
      "video 1/1 (frame 85/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 94.5ms\n",
      "video 1/1 (frame 86/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 15 Whiteflys, 96.5ms\n",
      "video 1/1 (frame 87/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 15 Whiteflys, 94.6ms\n",
      "video 1/1 (frame 88/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 96.2ms\n",
      "video 1/1 (frame 89/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 15 Whiteflys, 98.6ms\n",
      "video 1/1 (frame 90/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 92.1ms\n",
      "video 1/1 (frame 91/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 95.3ms\n",
      "video 1/1 (frame 92/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 15 Whiteflys, 98.3ms\n",
      "video 1/1 (frame 93/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 12 Whiteflys, 98.5ms\n",
      "video 1/1 (frame 94/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 14 Whiteflys, 100.1ms\n",
      "video 1/1 (frame 95/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 96.5ms\n",
      "video 1/1 (frame 96/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 98.5ms\n",
      "video 1/1 (frame 97/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 100.2ms\n",
      "video 1/1 (frame 98/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 96.7ms\n",
      "video 1/1 (frame 99/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 98.3ms\n",
      "video 1/1 (frame 100/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 100.8ms\n",
      "video 1/1 (frame 101/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 96.0ms\n",
      "video 1/1 (frame 102/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 96.1ms\n",
      "video 1/1 (frame 103/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 103.3ms\n",
      "video 1/1 (frame 104/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 105/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.6ms\n",
      "video 1/1 (frame 106/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 96.9ms\n",
      "video 1/1 (frame 107/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 95.5ms\n",
      "video 1/1 (frame 108/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 109/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 99.4ms\n",
      "video 1/1 (frame 110/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 100.4ms\n",
      "video 1/1 (frame 111/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 96.3ms\n",
      "video 1/1 (frame 112/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 98.6ms\n",
      "video 1/1 (frame 113/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 96.8ms\n",
      "video 1/1 (frame 114/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 115/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 93.6ms\n",
      "video 1/1 (frame 116/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 94.5ms\n",
      "video 1/1 (frame 117/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 100.4ms\n",
      "video 1/1 (frame 118/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 101.9ms\n",
      "video 1/1 (frame 119/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 95.2ms\n",
      "video 1/1 (frame 120/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 95.8ms\n",
      "video 1/1 (frame 121/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.3ms\n",
      "video 1/1 (frame 122/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 95.1ms\n",
      "video 1/1 (frame 123/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 94.3ms\n",
      "video 1/1 (frame 124/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 97.5ms\n",
      "video 1/1 (frame 125/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 96.3ms\n",
      "video 1/1 (frame 126/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 94.9ms\n",
      "video 1/1 (frame 127/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 96.3ms\n",
      "video 1/1 (frame 128/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.9ms\n",
      "video 1/1 (frame 129/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 98.0ms\n",
      "video 1/1 (frame 130/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 98.3ms\n",
      "video 1/1 (frame 131/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 96.9ms\n",
      "video 1/1 (frame 132/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 96.2ms\n",
      "video 1/1 (frame 133/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 96.1ms\n",
      "video 1/1 (frame 134/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 91.7ms\n",
      "video 1/1 (frame 135/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 92.7ms\n",
      "video 1/1 (frame 136/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 98.0ms\n",
      "video 1/1 (frame 137/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 100.5ms\n",
      "video 1/1 (frame 138/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 99.2ms\n",
      "video 1/1 (frame 139/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.0ms\n",
      "video 1/1 (frame 140/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.5ms\n",
      "video 1/1 (frame 141/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 99.8ms\n",
      "video 1/1 (frame 142/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 103.5ms\n",
      "video 1/1 (frame 143/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 99.1ms\n",
      "video 1/1 (frame 144/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 100.4ms\n",
      "video 1/1 (frame 145/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 96.2ms\n",
      "video 1/1 (frame 146/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 97.4ms\n",
      "video 1/1 (frame 147/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 98.4ms\n",
      "video 1/1 (frame 148/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 149/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 97.7ms\n",
      "video 1/1 (frame 150/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 151/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 97.2ms\n",
      "video 1/1 (frame 152/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 98.7ms\n",
      "video 1/1 (frame 153/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 100.7ms\n",
      "video 1/1 (frame 154/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 110.7ms\n",
      "video 1/1 (frame 155/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 120.7ms\n",
      "video 1/1 (frame 156/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 111.1ms\n",
      "video 1/1 (frame 157/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 98.9ms\n",
      "video 1/1 (frame 158/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 13 Whiteflys, 105.8ms\n",
      "video 1/1 (frame 159/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 123.8ms\n",
      "video 1/1 (frame 160/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 106.6ms\n",
      "video 1/1 (frame 161/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 110.7ms\n",
      "video 1/1 (frame 162/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 98.9ms\n",
      "video 1/1 (frame 163/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 98.1ms\n",
      "video 1/1 (frame 164/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 171.0ms\n",
      "video 1/1 (frame 165/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 117.6ms\n",
      "video 1/1 (frame 166/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 107.0ms\n",
      "video 1/1 (frame 167/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 101.2ms\n",
      "video 1/1 (frame 168/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 101.5ms\n",
      "video 1/1 (frame 169/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 100.8ms\n",
      "video 1/1 (frame 170/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 103.3ms\n",
      "video 1/1 (frame 171/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 104.7ms\n",
      "video 1/1 (frame 172/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 99.4ms\n",
      "video 1/1 (frame 173/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 100.0ms\n",
      "video 1/1 (frame 174/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 114.9ms\n",
      "video 1/1 (frame 175/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 115.5ms\n",
      "video 1/1 (frame 176/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 100.9ms\n",
      "video 1/1 (frame 177/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 104.1ms\n",
      "video 1/1 (frame 178/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 101.8ms\n",
      "video 1/1 (frame 179/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 129.1ms\n",
      "video 1/1 (frame 180/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 97.3ms\n",
      "video 1/1 (frame 181/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 99.9ms\n",
      "video 1/1 (frame 182/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 101.6ms\n",
      "video 1/1 (frame 183/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 96.3ms\n",
      "video 1/1 (frame 184/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 103.4ms\n",
      "video 1/1 (frame 185/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 95.7ms\n",
      "video 1/1 (frame 186/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 98.5ms\n",
      "video 1/1 (frame 187/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 99.1ms\n",
      "video 1/1 (frame 188/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 99.1ms\n",
      "video 1/1 (frame 189/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 101.6ms\n",
      "video 1/1 (frame 190/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 99.3ms\n",
      "video 1/1 (frame 191/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 103.8ms\n",
      "video 1/1 (frame 192/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 96.7ms\n",
      "video 1/1 (frame 193/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 194/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 101.0ms\n",
      "video 1/1 (frame 195/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 100.9ms\n",
      "video 1/1 (frame 196/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 197/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 103.6ms\n",
      "video 1/1 (frame 198/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 132.0ms\n",
      "video 1/1 (frame 199/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 114.8ms\n",
      "video 1/1 (frame 200/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 105.3ms\n",
      "video 1/1 (frame 201/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 114.7ms\n",
      "video 1/1 (frame 202/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 104.2ms\n",
      "video 1/1 (frame 203/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 100.4ms\n",
      "video 1/1 (frame 204/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 100.9ms\n",
      "video 1/1 (frame 205/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 96.3ms\n",
      "video 1/1 (frame 206/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 207/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 100.1ms\n",
      "video 1/1 (frame 208/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 100.4ms\n",
      "video 1/1 (frame 209/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 96.6ms\n",
      "video 1/1 (frame 210/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 97.2ms\n",
      "video 1/1 (frame 211/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 99.0ms\n",
      "video 1/1 (frame 212/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 97.8ms\n",
      "video 1/1 (frame 213/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 94.3ms\n",
      "video 1/1 (frame 214/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 112.6ms\n",
      "video 1/1 (frame 215/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 115.4ms\n",
      "video 1/1 (frame 216/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 111.8ms\n",
      "video 1/1 (frame 217/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 112.6ms\n",
      "video 1/1 (frame 218/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 122.6ms\n",
      "video 1/1 (frame 219/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 111.4ms\n",
      "video 1/1 (frame 220/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 111.2ms\n",
      "video 1/1 (frame 221/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 107.6ms\n",
      "video 1/1 (frame 222/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 116.4ms\n",
      "video 1/1 (frame 223/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 118.5ms\n",
      "video 1/1 (frame 224/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 117.8ms\n",
      "video 1/1 (frame 225/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 108.3ms\n",
      "video 1/1 (frame 226/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 115.3ms\n",
      "video 1/1 (frame 227/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 112.3ms\n",
      "video 1/1 (frame 228/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 109.8ms\n",
      "video 1/1 (frame 229/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 114.6ms\n",
      "video 1/1 (frame 230/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 112.4ms\n",
      "video 1/1 (frame 231/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 109.8ms\n",
      "video 1/1 (frame 232/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 111.6ms\n",
      "video 1/1 (frame 233/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 113.5ms\n",
      "video 1/1 (frame 234/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 108.8ms\n",
      "video 1/1 (frame 235/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 112.0ms\n",
      "video 1/1 (frame 236/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 115.7ms\n",
      "video 1/1 (frame 237/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 114.4ms\n",
      "video 1/1 (frame 238/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 117.9ms\n",
      "video 1/1 (frame 239/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 111.0ms\n",
      "video 1/1 (frame 240/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 118.0ms\n",
      "video 1/1 (frame 241/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 108.1ms\n",
      "video 1/1 (frame 242/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 109.6ms\n",
      "video 1/1 (frame 243/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 110.0ms\n",
      "video 1/1 (frame 244/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 114.0ms\n",
      "video 1/1 (frame 245/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 113.9ms\n",
      "video 1/1 (frame 246/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 115.6ms\n",
      "video 1/1 (frame 247/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 117.6ms\n",
      "video 1/1 (frame 248/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 112.7ms\n",
      "video 1/1 (frame 249/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 108.4ms\n",
      "video 1/1 (frame 250/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 115.5ms\n",
      "video 1/1 (frame 251/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 110.6ms\n",
      "video 1/1 (frame 252/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 112.0ms\n",
      "video 1/1 (frame 253/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 104.7ms\n",
      "video 1/1 (frame 254/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 112.5ms\n",
      "video 1/1 (frame 255/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 97.2ms\n",
      "video 1/1 (frame 256/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 93.4ms\n",
      "video 1/1 (frame 257/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 103.5ms\n",
      "video 1/1 (frame 258/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 259/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 94.1ms\n",
      "video 1/1 (frame 260/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 101.4ms\n",
      "video 1/1 (frame 261/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 97.4ms\n",
      "video 1/1 (frame 262/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 94.9ms\n",
      "video 1/1 (frame 263/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 106.6ms\n",
      "video 1/1 (frame 264/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 96.8ms\n",
      "video 1/1 (frame 265/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 98.2ms\n",
      "video 1/1 (frame 266/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 96.6ms\n",
      "video 1/1 (frame 267/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 109.1ms\n",
      "video 1/1 (frame 268/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 107.6ms\n",
      "video 1/1 (frame 269/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 118.4ms\n",
      "video 1/1 (frame 270/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 116.6ms\n",
      "video 1/1 (frame 271/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 99.3ms\n",
      "video 1/1 (frame 272/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 110.2ms\n",
      "video 1/1 (frame 273/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 101.3ms\n",
      "video 1/1 (frame 274/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 110.0ms\n",
      "video 1/1 (frame 275/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 96.8ms\n",
      "video 1/1 (frame 276/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 98.6ms\n",
      "video 1/1 (frame 277/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 278/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 98.0ms\n",
      "video 1/1 (frame 279/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 97.6ms\n",
      "video 1/1 (frame 280/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 97.5ms\n",
      "video 1/1 (frame 281/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 11 Whiteflys, 96.8ms\n",
      "video 1/1 (frame 282/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 98.1ms\n",
      "video 1/1 (frame 283/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 99.5ms\n",
      "video 1/1 (frame 284/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 95.7ms\n",
      "video 1/1 (frame 285/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 95.6ms\n",
      "video 1/1 (frame 286/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 96.8ms\n",
      "video 1/1 (frame 287/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.4ms\n",
      "video 1/1 (frame 288/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 99.6ms\n",
      "video 1/1 (frame 289/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 96.7ms\n",
      "video 1/1 (frame 290/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 97.6ms\n",
      "video 1/1 (frame 291/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 95.7ms\n",
      "video 1/1 (frame 292/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 98.0ms\n",
      "video 1/1 (frame 293/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 102.2ms\n",
      "video 1/1 (frame 294/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 106.9ms\n",
      "video 1/1 (frame 295/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 95.4ms\n",
      "video 1/1 (frame 296/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.2ms\n",
      "video 1/1 (frame 297/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 97.8ms\n",
      "video 1/1 (frame 298/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 95.9ms\n",
      "video 1/1 (frame 299/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 95.1ms\n",
      "video 1/1 (frame 300/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 95.6ms\n",
      "video 1/1 (frame 301/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 113.2ms\n",
      "video 1/1 (frame 302/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 10 Whiteflys, 98.6ms\n",
      "video 1/1 (frame 303/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 304/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 95.3ms\n",
      "video 1/1 (frame 305/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 96.1ms\n",
      "video 1/1 (frame 306/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 98.8ms\n",
      "video 1/1 (frame 307/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 308/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 98.9ms\n",
      "video 1/1 (frame 309/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 94.4ms\n",
      "video 1/1 (frame 310/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 97.7ms\n",
      "video 1/1 (frame 311/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 312/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.7ms\n",
      "video 1/1 (frame 313/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.0ms\n",
      "video 1/1 (frame 314/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 97.0ms\n",
      "video 1/1 (frame 315/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 316/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 100.4ms\n",
      "video 1/1 (frame 317/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 104.4ms\n",
      "video 1/1 (frame 318/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 96.5ms\n",
      "video 1/1 (frame 319/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 96.9ms\n",
      "video 1/1 (frame 320/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 101.6ms\n",
      "video 1/1 (frame 321/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 99.0ms\n",
      "video 1/1 (frame 322/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 9 Whiteflys, 95.4ms\n",
      "video 1/1 (frame 323/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 95.1ms\n",
      "video 1/1 (frame 324/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 8 Whiteflys, 97.8ms\n",
      "video 1/1 (frame 325/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 97.7ms\n",
      "video 1/1 (frame 326/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 95.7ms\n",
      "video 1/1 (frame 327/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 7 Whiteflys, 98.2ms\n",
      "video 1/1 (frame 328/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 95.5ms\n",
      "video 1/1 (frame 329/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 97.5ms\n",
      "video 1/1 (frame 330/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 96.0ms\n",
      "video 1/1 (frame 331/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 96.7ms\n",
      "video 1/1 (frame 332/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 96.8ms\n",
      "video 1/1 (frame 333/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 95.4ms\n",
      "video 1/1 (frame 334/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 95.2ms\n",
      "video 1/1 (frame 335/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 95.9ms\n",
      "video 1/1 (frame 336/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 99.4ms\n",
      "video 1/1 (frame 337/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 101.3ms\n",
      "video 1/1 (frame 338/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 93.9ms\n",
      "video 1/1 (frame 339/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 340/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 102.7ms\n",
      "video 1/1 (frame 341/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 95.9ms\n",
      "video 1/1 (frame 342/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 98.7ms\n",
      "video 1/1 (frame 343/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 96.8ms\n",
      "video 1/1 (frame 344/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 97.9ms\n",
      "video 1/1 (frame 345/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 6 Whiteflys, 98.6ms\n",
      "video 1/1 (frame 346/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 99.6ms\n",
      "video 1/1 (frame 347/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 1 Whitefly, 98.0ms\n",
      "video 1/1 (frame 348/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 1 Whitefly, 101.6ms\n",
      "video 1/1 (frame 349/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 350/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 99.9ms\n",
      "video 1/1 (frame 351/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 95.9ms\n",
      "video 1/1 (frame 352/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 4 Whiteflys, 98.4ms\n",
      "video 1/1 (frame 353/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 98.0ms\n",
      "video 1/1 (frame 354/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 97.6ms\n",
      "video 1/1 (frame 355/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 97.4ms\n",
      "video 1/1 (frame 356/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 1 Whitefly, 98.6ms\n",
      "video 1/1 (frame 357/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 101.9ms\n",
      "video 1/1 (frame 358/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 1 Whitefly, 97.5ms\n",
      "video 1/1 (frame 359/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 1 Whitefly, 96.5ms\n",
      "video 1/1 (frame 360/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 1 Whitefly, 97.3ms\n",
      "video 1/1 (frame 361/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 104.0ms\n",
      "video 1/1 (frame 362/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 100.9ms\n",
      "video 1/1 (frame 363/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 97.5ms\n",
      "video 1/1 (frame 364/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 5 Whiteflys, 99.8ms\n",
      "video 1/1 (frame 365/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 100.1ms\n",
      "video 1/1 (frame 366/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 98.0ms\n",
      "video 1/1 (frame 367/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 99.7ms\n",
      "video 1/1 (frame 368/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 100.1ms\n",
      "video 1/1 (frame 369/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 97.1ms\n",
      "video 1/1 (frame 370/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 100.0ms\n",
      "video 1/1 (frame 371/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 98.3ms\n",
      "video 1/1 (frame 372/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 97.0ms\n",
      "video 1/1 (frame 373/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 97.6ms\n",
      "video 1/1 (frame 374/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 98.2ms\n",
      "video 1/1 (frame 375/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 98.2ms\n",
      "video 1/1 (frame 376/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 175.1ms\n",
      "video 1/1 (frame 377/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 109.7ms\n",
      "video 1/1 (frame 378/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 105.0ms\n",
      "video 1/1 (frame 379/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 103.6ms\n",
      "video 1/1 (frame 380/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 98.8ms\n",
      "video 1/1 (frame 381/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 123.3ms\n",
      "video 1/1 (frame 382/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 (no detections), 98.7ms\n",
      "video 1/1 (frame 383/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 3 Whiteflys, 102.0ms\n",
      "video 1/1 (frame 384/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 1 Whitefly, 96.0ms\n",
      "video 1/1 (frame 385/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 (no detections), 101.6ms\n",
      "video 1/1 (frame 386/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 98.0ms\n",
      "video 1/1 (frame 387/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 2 Whiteflys, 96.3ms\n",
      "video 1/1 (frame 388/388) /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/examples/example_video.mp4: 736x1280 1 Whitefly, 99.1ms\n",
      "Speed: 3.6ms preprocess, 101.4ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 1280)\n",
      "Results saved to \u001b[1m.temp/detect/detect\u001b[0m\n",
      "386 labels saved to .temp/detect/detect/labels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\".temp/detect/detect/example_video.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import detect_yolo\n",
    "import os\n",
    "from IPython.display import Image, Video, display\n",
    "\n",
    "file_to_detect = 'examples/example_video.mp4'  # or use an image like 'dataset/improved/2.jpg'\n",
    "# file_to_detect = 'dataset/improved/2.jpg'\n",
    "\n",
    "# Run detection using the trained YOLO model\n",
    "results = detect_yolo(\n",
    "    source=file_to_detect,\n",
    "    weights=f\"{project}/{name}/weights/best.pt\",\n",
    "    img=image_size,\n",
    ")\n",
    "\n",
    "# Get the output path\n",
    "output_dir = results[0].save_dir  # YOLO typically saves to 'runs/detect/<name>/'\n",
    "file_name = os.path.basename(file_to_detect)\n",
    "output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# Display output based on extension\n",
    "if file_to_detect.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "    display(Image(filename=output_path))\n",
    "elif file_to_detect.lower().endswith(('.mp4', '.mov', '.avi')):\n",
    "    display(Video(filename=output_path))\n",
    "else:\n",
    "    print(f\"Unsupported file type: {file_to_detect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc443367",
   "metadata": {},
   "source": [
    "## 🤖 Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de08a0",
   "metadata": {},
   "source": [
    "The goal of **Active Learning (AL)** is to select the **most relevant samples** to be labeled first and used to train a model.  \n",
    "This approach is particularly useful when **labeling data is expensive or time-consuming**.\n",
    "\n",
    "Even with a **small initial set of labeled data**, it is possible to train a baseline model.  \n",
    "To improve the model further, we need more labeled data — but instead of labeling everything, **Active Learning helps prioritize** which samples to label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea14083",
   "metadata": {},
   "source": [
    "### Active Learning Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10317689",
   "metadata": {},
   "source": [
    "There are two main approaches to Active Learning:\n",
    "\n",
    "#### Stream-Based Selective Sampling\n",
    "\n",
    "- Unlabeled data points are processed **one at a time**.\n",
    "- For each sample, the model must decide **immediately** whether to query its label or discard it.\n",
    "- Useful when data arrives in real time or memory is limited.\n",
    "\n",
    "#### Pool-Based Sampling\n",
    "\n",
    "- The model evaluates a **pool of unlabeled data** all at once.\n",
    "- Each sample is scored based on a **relevance criterion** (e.g., confidence, entropy).\n",
    "- The **top-K most informative** samples are selected and labeled.\n",
    "- This is the most commonly used method in practice.\n",
    "\n",
    "In Pool-based the process works as follows:\n",
    "\n",
    "##### 1. A model is trained on the current labeled dataset.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/al_1.png)\n",
    "\n",
    "##### 2. The model is then used to evaluate the **unlabeled data**.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/al_2.png)\n",
    "\n",
    "##### 3. Based on its predictions, the model **scores all unlabeled samples** using a selection strategy (e.g., uncertainty, disagreement, diversity).\n",
    "   - Samples are then **ranked** by relevance.\n",
    "   - Common scoring strategies include:\n",
    "     - **Uncertainty sampling**: Select samples with the lowest confidence (highest uncertainty).\n",
    "     - **Diversity sampling**: Choose a diverse set of samples to maximize information gain and avoid redundancy.\n",
    "     - **Representativeness**: Pick samples that best represent the distribution of the overall data.\n",
    "     - **Entropy-based sampling**: Select samples with the highest prediction entropy (more confusion between classes).\n",
    "\n",
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/al_3.png)\n",
    "\n",
    "##### 4. Based on the ranking, the **top-K most relevant samples are selected, labeled by an expert, and added to the labeled set**.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/al_4.png)\n",
    "\n",
    "##### 5. This cycle repeats, improving the model efficiently while **minimizing annotation effort**.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/al_5.png)\n",
    "\n",
    "---\n",
    "\n",
    "By focusing labeling efforts on the **most informative samples**, Active Learning enables more efficient training and faster model improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be83b39",
   "metadata": {},
   "source": [
    "### Experiment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc164d",
   "metadata": {},
   "source": [
    "In this section, we evaluate the impact of **Active Learning (AL)** in a Pool-based scenario by training and comparing **two models**:\n",
    "\n",
    "##### Incremental Training Strategy\n",
    "\n",
    "Both models are trained **incrementally**:\n",
    "\n",
    "- Training begins with a **small subset** of images for training and validation.\n",
    "- At each iteration, a **new batch of data** is added to the training and validation sets.\n",
    "- A new model is trained after each update.\n",
    "- This process is repeated for a fixed number of iterations, defined by the variable `num_batches`.\n",
    "\n",
    "The number of images added in each iteration is determined by:\n",
    "\n",
    "- `batch_train_proportion * dataset_size` for training images\n",
    "- `batch_val_proportion * dataset_size` for validation images\n",
    "\n",
    "---\n",
    "\n",
    "##### Model Variants\n",
    "\n",
    "- **Active Learning Model:**  \n",
    "  Selects new images based on **model uncertainty**, e.g., low confidence in predictions. This ensures the **most informative** samples are used in training in first place.\n",
    "\n",
    "- **Random Selection Model:**  \n",
    "  Selects new images **randomly** from the unlabeled pool at each iteration, serving as a baseline for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "##### ⚙️ Key Parameters\n",
    "\n",
    "- `num_batches`: Number of incremental iterations (rounds of training)\n",
    "- `batch_train_proportion`: Proportion of the dataset used for training in each batch\n",
    "- `batch_val_proportion`: Proportion of the dataset used for validation in each batch\n",
    "\n",
    "These parameters control **how much data is added per iteration** and ensure both models grow in training size at the same pace, allowing for a fair comparison.\n",
    "\n",
    "---\n",
    "\n",
    "📈 At the end of the experiment, we compare the models using their **mAP@50** scores across iterations to evaluate which strategy learns more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9c01706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_split, split_dataset, new_batch, train_yolo, test_yolo\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# -------- Dataset setup --------\n",
    "# Define dataset parameters\n",
    "\n",
    "dataset_size = 200 # Total number of images in the dataset\n",
    "num_batches = 8 # Number of batches to process\n",
    "batch_train_prop = 0.6 / num_batches # Proportion of images in each batch for training (60% of the dataset)\n",
    "batch_val_prop = 0.2 / num_batches # Proportion of images in each batch for validation (20% of the dataset)\n",
    "\n",
    "num_epochs = 20 # Number of epochs for each training run\n",
    "batch_size = 4 # Batch size for training\n",
    "image_size = 1280 # Image size for detection\n",
    "\n",
    "# Project base\n",
    "project = \"example_active_learning\" # Base project directory for saving results\n",
    "base_weights = \"yolo11n.pt\" # Pre-trained weights to start training from\n",
    "\n",
    "results_df = pd.DataFrame(index=range(num_batches)) # DataFrame to store results\n",
    "\n",
    "# -------- Initial split --------\n",
    "# Get initial train/val split for the first batch \n",
    "# this will ensure both models start with the same initial data\n",
    "initial_train_split, initial_val_split = get_split(\n",
    "                train_size=batch_train_prop,\n",
    "                val_size=batch_val_prop,\n",
    "                dataset_size=dataset_size\n",
    "            ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2575a42c",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1154cdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Batch 0 / 8 — Mode: al ===\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1280, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=al_batch_0, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=example_active_learning_al/training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=example_active_learning_al/training/al_batch_0, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 8920.5±658.6 MB/s, size: 620.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train... 15 images, 0 backgrounds, 0 corrupt: 100%|██████████| 15/15 [00:00<00:00, 2604.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 9755.0±1676.2 MB/s, size: 767.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val... 5 images, 0 backgrounds, 0 corrupt: 100%|██████████| 5/5 [00:00<00:00, 3515.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val.cache\n",
      "Plotting labels to example_active_learning_al/training/al_batch_0/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mexample_active_learning_al/training/al_batch_0\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G      2.832      4.286      1.212        124       1280: 100%|██████████| 4/4 [00:15<00:00,  3.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20         0G      2.511      3.976      1.094        158       1280: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20         0G      2.185      3.658      1.027        196       1280: 100%|██████████| 4/4 [00:13<00:00,  3.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20         0G      1.898      3.398      1.012         93       1280: 100%|██████████| 4/4 [00:16<00:00,  4.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390    0.00667     0.0256    0.00956    0.00254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20         0G      1.813      3.016     0.9313        166       1280: 100%|██████████| 4/4 [00:14<00:00,  3.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390     0.0153      0.059     0.0415     0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20         0G      1.972      2.999     0.9465         85       1280: 100%|██████████| 4/4 [00:13<00:00,  3.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.024     0.0923     0.0749     0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20         0G      1.918      2.593     0.9274        250       1280: 100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.024     0.0923     0.0749     0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20         0G      1.826      2.351     0.9382        155       1280: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390     0.0273      0.105     0.0751     0.0203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20         0G      1.789      2.269     0.9222        212       1280: 100%|██████████| 4/4 [00:14<00:00,  3.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.034      0.131     0.0904      0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20         0G      1.769       2.05     0.9029        239       1280: 100%|██████████| 4/4 [00:13<00:00,  3.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.034      0.131     0.0904      0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20         0G      1.664      1.973     0.9269         63       1280: 100%|██████████| 4/4 [00:13<00:00,  3.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390       0.04      0.154      0.095     0.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20         0G      1.741      1.943     0.9159        186       1280: 100%|██████████| 4/4 [00:13<00:00,  3.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390       0.04      0.154      0.095     0.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20         0G      1.682      2.066     0.9276         95       1280: 100%|██████████| 4/4 [00:13<00:00,  3.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.054      0.208      0.116     0.0377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20         0G      1.716      1.878     0.9127        143       1280: 100%|██████████| 4/4 [00:13<00:00,  3.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.054      0.208      0.116     0.0377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20         0G      1.615      1.746     0.9176         77       1280: 100%|██████████| 4/4 [00:17<00:00,  4.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.092      0.354      0.277      0.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20         0G      1.578      1.718     0.9113        139       1280: 100%|██████████| 4/4 [00:14<00:00,  3.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.092      0.354      0.277      0.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20         0G      1.815      2.021     0.9411        142       1280: 100%|██████████| 4/4 [00:14<00:00,  3.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.092      0.354      0.277      0.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20         0G      1.669       1.78     0.9097        255       1280: 100%|██████████| 4/4 [00:13<00:00,  3.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.127       0.49      0.418      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20         0G      1.678      1.984     0.9277         80       1280: 100%|██████████| 4/4 [00:13<00:00,  3.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.127       0.49      0.418      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20         0G      1.523      1.695     0.8979        181       1280: 100%|██████████| 4/4 [00:13<00:00,  3.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.127       0.49      0.418      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.091 hours.\n",
      "Optimizer stripped from example_active_learning_al/training/al_batch_0/weights/last.pt, 5.6MB\n",
      "Optimizer stripped from example_active_learning_al/training/al_batch_0/weights/best.pt, 5.6MB\n",
      "\n",
      "Validating example_active_learning_al/training/al_batch_0/weights/best.pt...\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          5        390      0.127       0.49      0.418      0.222\n",
      "Speed: 2.9ms preprocess, 185.1ms inference, 0.0ms loss, 82.4ms postprocess per image\n",
      "Results saved to \u001b[1mexample_active_learning_al/training/al_batch_0\u001b[0m\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1259.2±160.7 MB/s, size: 770.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/test.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|██████████| 40/40 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:21<00:00,  7.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       2060     0.0824       0.48      0.373      0.195\n",
      "Speed: 2.1ms preprocess, 436.6ms inference, 0.0ms loss, 77.9ms postprocess per image\n",
      "Results saved to \u001b[1mexample_active_learning_al/testing/test\u001b[0m\n",
      "✅ Batch 1 (al) done — mAP@50: 0.4182\n",
      "\n",
      "=== Batch 1 / 8 — Mode: al ===\n",
      "Creating new batch with 15 train images and 5 validation images.\n",
      "Using active learning mode: confidence\n",
      "Available indices for new batch: 140\n",
      "Detection directory created at: dataset/detect\n",
      "\n",
      "image 1/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/0.jpg: 1280x1280 (no detections), 167.9ms\n",
      "image 2/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/1.jpg: 1280x1280 (no detections), 169.3ms\n",
      "image 3/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/10.jpg: 1280x1280 (no detections), 164.4ms\n",
      "image 4/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/101.jpg: 1280x1280 (no detections), 163.9ms\n",
      "image 5/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/102.jpg: 1280x1280 (no detections), 178.0ms\n",
      "image 6/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/103.jpg: 1280x1280 (no detections), 159.9ms\n",
      "image 7/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/104.jpg: 1280x1280 (no detections), 155.0ms\n",
      "image 8/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/106.jpg: 1280x1280 (no detections), 152.7ms\n",
      "image 9/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/107.jpg: 1280x1280 (no detections), 155.9ms\n",
      "image 10/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/108.jpg: 1280x1280 (no detections), 158.1ms\n",
      "image 11/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/109.jpg: 1280x1280 (no detections), 161.9ms\n",
      "image 12/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/11.jpg: 1280x1280 (no detections), 157.8ms\n",
      "image 13/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/110.jpg: 1280x1280 (no detections), 181.5ms\n",
      "image 14/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/111.jpg: 1280x1280 (no detections), 181.5ms\n",
      "image 15/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/112.jpg: 1280x1280 (no detections), 189.3ms\n",
      "image 16/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/113.jpg: 1280x1280 (no detections), 184.6ms\n",
      "image 17/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/114.jpg: 1280x1280 (no detections), 161.6ms\n",
      "image 18/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/115.jpg: 1280x1280 (no detections), 156.1ms\n",
      "image 19/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/118.jpg: 1280x1280 (no detections), 157.5ms\n",
      "image 20/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/119.jpg: 1280x1280 (no detections), 671.1ms\n",
      "image 21/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/120.jpg: 1280x1280 (no detections), 162.5ms\n",
      "image 22/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/121.jpg: 1280x1280 (no detections), 161.3ms\n",
      "image 23/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/122.jpg: 1280x1280 (no detections), 170.8ms\n",
      "image 24/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/123.jpg: 1280x1280 (no detections), 150.8ms\n",
      "image 25/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/125.jpg: 1280x1280 (no detections), 156.7ms\n",
      "image 26/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/126.jpg: 1280x1280 (no detections), 160.4ms\n",
      "image 27/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/127.jpg: 1280x1280 (no detections), 148.3ms\n",
      "image 28/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/128.jpg: 1280x1280 (no detections), 149.5ms\n",
      "image 29/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/134.jpg: 1280x1280 (no detections), 146.0ms\n",
      "image 30/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/135.jpg: 1280x1280 (no detections), 150.3ms\n",
      "image 31/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/136.jpg: 1280x1280 (no detections), 150.2ms\n",
      "image 32/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/137.jpg: 1280x1280 (no detections), 150.7ms\n",
      "image 33/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/14.jpg: 1280x1280 (no detections), 153.5ms\n",
      "image 34/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/140.jpg: 1280x1280 (no detections), 159.4ms\n",
      "image 35/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/141.jpg: 1280x1280 (no detections), 183.5ms\n",
      "image 36/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/143.jpg: 1280x1280 (no detections), 161.8ms\n",
      "image 37/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/144.jpg: 1280x1280 (no detections), 162.7ms\n",
      "image 38/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/146.jpg: 1280x1280 (no detections), 155.9ms\n",
      "image 39/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/148.jpg: 1280x1280 (no detections), 167.7ms\n",
      "image 40/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/149.jpg: 1280x1280 (no detections), 152.9ms\n",
      "image 41/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/15.jpg: 1280x1280 (no detections), 154.0ms\n",
      "image 42/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/150.jpg: 1280x1280 (no detections), 175.6ms\n",
      "image 43/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/152.jpg: 1280x1280 (no detections), 151.4ms\n",
      "image 44/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/153.jpg: 1280x1280 (no detections), 155.3ms\n",
      "image 45/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/154.jpg: 1280x1280 (no detections), 166.2ms\n",
      "image 46/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/155.jpg: 1280x1280 (no detections), 169.9ms\n",
      "image 47/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/156.jpg: 1280x1280 (no detections), 164.3ms\n",
      "image 48/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/157.jpg: 1280x1280 (no detections), 144.1ms\n",
      "image 49/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/160.jpg: 1280x1280 (no detections), 181.8ms\n",
      "image 50/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/162.jpg: 1280x1280 (no detections), 173.7ms\n",
      "image 51/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/163.jpg: 1280x1280 (no detections), 149.6ms\n",
      "image 52/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/164.jpg: 1280x1280 (no detections), 160.7ms\n",
      "image 53/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/166.jpg: 1280x1280 (no detections), 180.7ms\n",
      "image 54/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/167.jpg: 1280x1280 (no detections), 161.0ms\n",
      "image 55/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/168.jpg: 1280x1280 (no detections), 154.3ms\n",
      "image 56/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/17.jpg: 1280x1280 (no detections), 150.0ms\n",
      "image 57/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/170.jpg: 1280x1280 (no detections), 145.5ms\n",
      "image 58/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/175.jpg: 1280x1280 (no detections), 144.2ms\n",
      "image 59/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/176.jpg: 1280x1280 (no detections), 145.7ms\n",
      "image 60/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/177.jpg: 1280x1280 (no detections), 142.3ms\n",
      "image 61/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/178.jpg: 1280x1280 (no detections), 143.4ms\n",
      "image 62/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/179.jpg: 1280x1280 (no detections), 144.1ms\n",
      "image 63/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/181.jpg: 1280x1280 (no detections), 147.2ms\n",
      "image 64/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/182.jpg: 1280x1280 (no detections), 143.9ms\n",
      "image 65/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/183.jpg: 1280x1280 (no detections), 142.4ms\n",
      "image 66/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/184.jpg: 1280x1280 (no detections), 145.9ms\n",
      "image 67/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/186.jpg: 1280x1280 (no detections), 144.5ms\n",
      "image 68/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/187.jpg: 1280x1280 (no detections), 143.9ms\n",
      "image 69/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/19.jpg: 1280x1280 (no detections), 157.3ms\n",
      "image 70/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/190.jpg: 1280x1280 (no detections), 159.3ms\n",
      "image 71/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/191.jpg: 1280x1280 (no detections), 150.5ms\n",
      "image 72/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/192.jpg: 1280x1280 (no detections), 148.9ms\n",
      "image 73/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/194.jpg: 1280x1280 (no detections), 147.7ms\n",
      "image 74/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/195.jpg: 1280x1280 (no detections), 142.8ms\n",
      "image 75/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/197.jpg: 1280x1280 (no detections), 142.7ms\n",
      "image 76/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/198.jpg: 1280x1280 (no detections), 145.4ms\n",
      "image 77/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/199.jpg: 1280x1280 (no detections), 145.0ms\n",
      "image 78/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/2.jpg: 1280x1280 (no detections), 148.0ms\n",
      "image 79/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/20.jpg: 1280x1280 (no detections), 155.6ms\n",
      "image 80/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/22.jpg: 1280x1280 (no detections), 147.0ms\n",
      "image 81/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/23.jpg: 1280x1280 (no detections), 149.3ms\n",
      "image 82/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/25.jpg: 1280x1280 (no detections), 145.5ms\n",
      "image 83/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/27.jpg: 1280x1280 (no detections), 145.7ms\n",
      "image 84/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/29.jpg: 1280x1280 (no detections), 145.8ms\n",
      "image 85/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/3.jpg: 1280x1280 (no detections), 143.9ms\n",
      "image 86/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/31.jpg: 1280x1280 (no detections), 151.4ms\n",
      "image 87/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/34.jpg: 1280x1280 (no detections), 141.6ms\n",
      "image 88/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/36.jpg: 1280x1280 (no detections), 150.1ms\n",
      "image 89/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/37.jpg: 1280x1280 (no detections), 145.9ms\n",
      "image 90/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/38.jpg: 1280x1280 (no detections), 149.1ms\n",
      "image 91/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/39.jpg: 1280x1280 (no detections), 141.9ms\n",
      "image 92/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/4.jpg: 1280x1280 (no detections), 140.7ms\n",
      "image 93/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/40.jpg: 1280x1280 (no detections), 146.0ms\n",
      "image 94/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/42.jpg: 1280x1280 (no detections), 140.1ms\n",
      "image 95/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/43.jpg: 1280x1280 (no detections), 139.7ms\n",
      "image 96/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/44.jpg: 1280x1280 (no detections), 144.9ms\n",
      "image 97/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/45.jpg: 1280x1280 (no detections), 145.9ms\n",
      "image 98/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/46.jpg: 1280x1280 (no detections), 146.2ms\n",
      "image 99/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/48.jpg: 1280x1280 (no detections), 193.5ms\n",
      "image 100/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/49.jpg: 1280x1280 (no detections), 240.1ms\n",
      "image 101/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/5.jpg: 1280x1280 (no detections), 151.6ms\n",
      "image 102/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/50.jpg: 1280x1280 (no detections), 149.5ms\n",
      "image 103/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/52.jpg: 1280x1280 (no detections), 142.8ms\n",
      "image 104/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/53.jpg: 1280x1280 (no detections), 150.3ms\n",
      "image 105/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/57.jpg: 1280x1280 (no detections), 148.7ms\n",
      "image 106/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/58.jpg: 1280x1280 (no detections), 154.3ms\n",
      "image 107/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/59.jpg: 1280x1280 (no detections), 146.8ms\n",
      "image 108/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/60.jpg: 1280x1280 (no detections), 147.3ms\n",
      "image 109/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/62.jpg: 1280x1280 (no detections), 158.3ms\n",
      "image 110/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/63.jpg: 1280x1280 (no detections), 148.0ms\n",
      "image 111/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/65.jpg: 1280x1280 (no detections), 148.9ms\n",
      "image 112/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/66.jpg: 1280x1280 (no detections), 143.5ms\n",
      "image 113/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/68.jpg: 1280x1280 (no detections), 143.6ms\n",
      "image 114/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/69.jpg: 1280x1280 (no detections), 153.1ms\n",
      "image 115/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/7.jpg: 1280x1280 (no detections), 144.5ms\n",
      "image 116/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/70.jpg: 1280x1280 (no detections), 144.7ms\n",
      "image 117/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/72.jpg: 1280x1280 (no detections), 147.6ms\n",
      "image 118/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/73.jpg: 1280x1280 (no detections), 142.6ms\n",
      "image 119/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/74.jpg: 1280x1280 (no detections), 144.6ms\n",
      "image 120/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/75.jpg: 1280x1280 (no detections), 160.2ms\n",
      "image 121/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/76.jpg: 1280x1280 (no detections), 142.5ms\n",
      "image 122/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/78.jpg: 1280x1280 (no detections), 145.4ms\n",
      "image 123/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/79.jpg: 1280x1280 (no detections), 143.3ms\n",
      "image 124/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/8.jpg: 1280x1280 (no detections), 151.8ms\n",
      "image 125/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/80.jpg: 1280x1280 (no detections), 144.5ms\n",
      "image 126/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/82.jpg: 1280x1280 (no detections), 187.8ms\n",
      "image 127/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/84.jpg: 1280x1280 (no detections), 150.7ms\n",
      "image 128/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/85.jpg: 1280x1280 (no detections), 147.8ms\n",
      "image 129/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/86.jpg: 1280x1280 (no detections), 150.5ms\n",
      "image 130/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/88.jpg: 1280x1280 (no detections), 171.0ms\n",
      "image 131/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/9.jpg: 1280x1280 (no detections), 152.9ms\n",
      "image 132/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/91.jpg: 1280x1280 (no detections), 147.2ms\n",
      "image 133/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/92.jpg: 1280x1280 (no detections), 150.0ms\n",
      "image 134/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/93.jpg: 1280x1280 (no detections), 147.4ms\n",
      "image 135/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/94.jpg: 1280x1280 (no detections), 148.9ms\n",
      "image 136/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/95.jpg: 1280x1280 (no detections), 148.1ms\n",
      "image 137/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/96.jpg: 1280x1280 (no detections), 167.9ms\n",
      "image 138/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/97.jpg: 1280x1280 (no detections), 153.3ms\n",
      "image 139/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/98.jpg: 1280x1280 (no detections), 144.2ms\n",
      "image 140/140 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/99.jpg: 1280x1280 (no detections), 143.5ms\n",
      "Speed: 7.7ms preprocess, 158.3ms inference, 0.7ms postprocess per image at shape (1, 3, 1280, 1280)\n",
      "Results saved to \u001b[1mexample_active_learning_al/detecting/al_batch_1\u001b[0m\n",
      "0 label saved to example_active_learning_al/detecting/al_batch_1/labels\n",
      "Available indices for new batch: 140\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1280, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=al_batch_1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=example_active_learning_al/training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=example_active_learning_al/training/al_batch_1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 3373.9±4011.1 MB/s, size: 902.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train... 30 images, 2 backgrounds, 0 corrupt: 100%|██████████| 30/30 [00:00<00:00, 1870.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 3696.3±4308.0 MB/s, size: 849.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val... 10 images, 0 backgrounds, 0 corrupt: 100%|██████████| 10/10 [00:00<00:00, 1828.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val.cache\n",
      "Plotting labels to example_active_learning_al/training/al_batch_1/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mexample_active_learning_al/training/al_batch_1\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G      2.777      4.058      1.167        158       1280: 100%|██████████| 8/8 [00:28<00:00,  3.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585   0.000333    0.00171   0.000187   7.47e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20         0G      2.238      3.611      1.061         92       1280: 100%|██████████| 8/8 [00:25<00:00,  3.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.008      0.041    0.00901    0.00218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20         0G       2.13      3.085     0.9866        100       1280: 100%|██████████| 8/8 [00:24<00:00,  3.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585     0.0113     0.0581    0.00872    0.00208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20         0G      2.242      2.751     0.9881         51       1280: 100%|██████████| 8/8 [00:24<00:00,  3.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585     0.0107     0.0547    0.00863     0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20         0G      2.027      2.111     0.9555        103       1280: 100%|██████████| 8/8 [00:24<00:00,  3.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585    0.00967     0.0496     0.0107    0.00352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20         0G      2.058      2.059      0.968        166       1280: 100%|██████████| 8/8 [00:28<00:00,  3.56s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585    0.00833     0.0427    0.00529    0.00209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20         0G      2.007      1.914     0.9544         90       1280: 100%|██████████| 8/8 [00:29<00:00,  3.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.055      0.282      0.186     0.0789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20         0G      2.089      2.242     0.9444         26       1280: 100%|██████████| 8/8 [00:27<00:00,  3.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585     0.0847      0.434      0.282     0.0961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20         0G      1.988      2.128     0.9696         36       1280: 100%|██████████| 8/8 [00:24<00:00,  3.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.124      0.636      0.388      0.154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20         0G      1.965      1.826     0.9767         98       1280: 100%|██████████| 8/8 [00:25<00:00,  3.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.124      0.636      0.388      0.154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20         0G      2.029       2.21     0.9896         57       1280: 100%|██████████| 8/8 [00:24<00:00,  3.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.203      0.771      0.494      0.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20         0G      2.146      2.212     0.9774        103       1280: 100%|██████████| 8/8 [00:25<00:00,  3.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.203      0.771      0.494      0.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20         0G      1.812       1.89      0.997         61       1280: 100%|██████████| 8/8 [00:24<00:00,  3.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.656      0.472      0.545      0.254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20         0G      1.854      1.829     0.9894        106       1280: 100%|██████████| 8/8 [00:25<00:00,  3.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.656      0.472      0.545      0.254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20         0G      1.957       2.12     0.9805        100       1280: 100%|██████████| 8/8 [00:25<00:00,  3.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585       0.67      0.482      0.594      0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20         0G      1.803      1.863      0.945         46       1280: 100%|██████████| 8/8 [00:25<00:00,  3.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585       0.67      0.482      0.594      0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20         0G      1.812      1.896      0.955         43       1280: 100%|██████████| 8/8 [00:30<00:00,  3.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585       0.64      0.554      0.625      0.314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20         0G      1.797      1.817     0.9727        122       1280: 100%|██████████| 8/8 [00:30<00:00,  3.81s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585       0.64      0.554      0.625      0.314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20         0G      1.714      1.745     0.9681         36       1280: 100%|██████████| 8/8 [00:29<00:00,  3.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.649      0.591      0.657      0.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20         0G      1.718      1.732     0.9694         70       1280: 100%|██████████| 8/8 [00:28<00:00,  3.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.649      0.591      0.657      0.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.171 hours.\n",
      "Optimizer stripped from example_active_learning_al/training/al_batch_1/weights/last.pt, 5.6MB\n",
      "Optimizer stripped from example_active_learning_al/training/al_batch_1/weights/best.pt, 5.6MB\n",
      "\n",
      "Validating example_active_learning_al/training/al_batch_1/weights/best.pt...\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         10        585      0.647      0.591      0.658      0.336\n",
      "Speed: 3.9ms preprocess, 194.1ms inference, 0.0ms loss, 178.6ms postprocess per image\n",
      "Results saved to \u001b[1mexample_active_learning_al/training/al_batch_1\u001b[0m\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1446.9±337.0 MB/s, size: 780.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/test.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|██████████| 40/40 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.800s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  33%|███▎      | 1/3 [00:11<00:23, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.800s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:26<00:00,  8.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       2060      0.653      0.387      0.446      0.248\n",
      "Speed: 3.0ms preprocess, 449.6ms inference, 0.0ms loss, 168.2ms postprocess per image\n",
      "Results saved to \u001b[1mexample_active_learning_al/testing/test\u001b[0m\n",
      "✅ Batch 2 (al) done — mAP@50: 0.6575\n",
      "\n",
      "=== Batch 2 / 8 — Mode: al ===\n",
      "Creating new batch with 15 train images and 5 validation images.\n",
      "Using active learning mode: confidence\n",
      "Available indices for new batch: 120\n",
      "Detection directory created at: dataset/detect\n",
      "\n",
      "image 1/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/0.jpg: 1280x1280 10 Whiteflys, 162.3ms\n",
      "image 2/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/1.jpg: 1280x1280 10 Whiteflys, 168.4ms\n",
      "image 3/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/10.jpg: 1280x1280 3 Whiteflys, 167.4ms\n",
      "image 4/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/101.jpg: 1280x1280 9 Whiteflys, 165.3ms\n",
      "image 5/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/102.jpg: 1280x1280 8 Whiteflys, 165.7ms\n",
      "image 6/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/103.jpg: 1280x1280 3 Whiteflys, 161.8ms\n",
      "image 7/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/104.jpg: 1280x1280 25 Whiteflys, 161.5ms\n",
      "image 8/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/106.jpg: 1280x1280 6 Whiteflys, 158.2ms\n",
      "image 9/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/107.jpg: 1280x1280 40 Whiteflys, 157.0ms\n",
      "image 10/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/108.jpg: 1280x1280 20 Whiteflys, 158.2ms\n",
      "image 11/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/109.jpg: 1280x1280 8 Whiteflys, 154.3ms\n",
      "image 12/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/11.jpg: 1280x1280 63 Whiteflys, 148.8ms\n",
      "image 13/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/110.jpg: 1280x1280 16 Whiteflys, 143.3ms\n",
      "image 14/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/111.jpg: 1280x1280 37 Whiteflys, 141.4ms\n",
      "image 15/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/112.jpg: 1280x1280 15 Whiteflys, 140.6ms\n",
      "image 16/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/113.jpg: 1280x1280 13 Whiteflys, 146.9ms\n",
      "image 17/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/114.jpg: 1280x1280 8 Whiteflys, 139.0ms\n",
      "image 18/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/115.jpg: 1280x1280 11 Whiteflys, 141.7ms\n",
      "image 19/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/118.jpg: 1280x1280 17 Whiteflys, 140.3ms\n",
      "image 20/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/119.jpg: 1280x1280 7 Whiteflys, 140.8ms\n",
      "image 21/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/120.jpg: 1280x1280 22 Whiteflys, 139.9ms\n",
      "image 22/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/121.jpg: 1280x1280 10 Whiteflys, 143.3ms\n",
      "image 23/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/122.jpg: 1280x1280 7 Whiteflys, 140.5ms\n",
      "image 24/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/123.jpg: 1280x1280 26 Whiteflys, 141.1ms\n",
      "image 25/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/125.jpg: 1280x1280 27 Whiteflys, 139.8ms\n",
      "image 26/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/126.jpg: 1280x1280 3 Whiteflys, 141.8ms\n",
      "image 27/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/127.jpg: 1280x1280 12 Whiteflys, 140.4ms\n",
      "image 28/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/128.jpg: 1280x1280 11 Whiteflys, 138.9ms\n",
      "image 29/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/134.jpg: 1280x1280 30 Whiteflys, 142.4ms\n",
      "image 30/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/135.jpg: 1280x1280 12 Whiteflys, 141.7ms\n",
      "image 31/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/136.jpg: 1280x1280 19 Whiteflys, 141.5ms\n",
      "image 32/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/137.jpg: 1280x1280 4 Whiteflys, 148.6ms\n",
      "image 33/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/14.jpg: 1280x1280 9 Whiteflys, 142.6ms\n",
      "image 34/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/140.jpg: 1280x1280 44 Whiteflys, 142.5ms\n",
      "image 35/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/141.jpg: 1280x1280 8 Whiteflys, 139.6ms\n",
      "image 36/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/143.jpg: 1280x1280 48 Whiteflys, 143.2ms\n",
      "image 37/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/144.jpg: 1280x1280 41 Whiteflys, 147.4ms\n",
      "image 38/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/146.jpg: 1280x1280 27 Whiteflys, 144.4ms\n",
      "image 39/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/15.jpg: 1280x1280 50 Whiteflys, 142.6ms\n",
      "image 40/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/150.jpg: 1280x1280 27 Whiteflys, 142.4ms\n",
      "image 41/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/152.jpg: 1280x1280 14 Whiteflys, 142.6ms\n",
      "image 42/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/153.jpg: 1280x1280 1 Whitefly, 142.1ms\n",
      "image 43/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/154.jpg: 1280x1280 9 Whiteflys, 140.3ms\n",
      "image 44/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/155.jpg: 1280x1280 40 Whiteflys, 144.2ms\n",
      "image 45/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/156.jpg: 1280x1280 10 Whiteflys, 194.4ms\n",
      "image 46/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/157.jpg: 1280x1280 38 Whiteflys, 143.2ms\n",
      "image 47/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/164.jpg: 1280x1280 26 Whiteflys, 143.0ms\n",
      "image 48/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/166.jpg: 1280x1280 30 Whiteflys, 141.2ms\n",
      "image 49/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/167.jpg: 1280x1280 1 Whitefly, 141.2ms\n",
      "image 50/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/168.jpg: 1280x1280 37 Whiteflys, 142.1ms\n",
      "image 51/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/17.jpg: 1280x1280 34 Whiteflys, 142.3ms\n",
      "image 52/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/178.jpg: 1280x1280 9 Whiteflys, 143.9ms\n",
      "image 53/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/179.jpg: 1280x1280 9 Whiteflys, 138.8ms\n",
      "image 54/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/181.jpg: 1280x1280 5 Whiteflys, 143.1ms\n",
      "image 55/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/182.jpg: 1280x1280 40 Whiteflys, 140.3ms\n",
      "image 56/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/183.jpg: 1280x1280 11 Whiteflys, 142.1ms\n",
      "image 57/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/184.jpg: 1280x1280 24 Whiteflys, 140.5ms\n",
      "image 58/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/186.jpg: 1280x1280 8 Whiteflys, 137.6ms\n",
      "image 59/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/187.jpg: 1280x1280 16 Whiteflys, 140.3ms\n",
      "image 60/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/19.jpg: 1280x1280 22 Whiteflys, 142.6ms\n",
      "image 61/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/190.jpg: 1280x1280 10 Whiteflys, 143.5ms\n",
      "image 62/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/191.jpg: 1280x1280 19 Whiteflys, 144.7ms\n",
      "image 63/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/192.jpg: 1280x1280 27 Whiteflys, 151.8ms\n",
      "image 64/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/194.jpg: 1280x1280 7 Whiteflys, 156.7ms\n",
      "image 65/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/195.jpg: 1280x1280 48 Whiteflys, 149.9ms\n",
      "image 66/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/197.jpg: 1280x1280 10 Whiteflys, 144.6ms\n",
      "image 67/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/198.jpg: 1280x1280 27 Whiteflys, 144.6ms\n",
      "image 68/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/199.jpg: 1280x1280 59 Whiteflys, 158.6ms\n",
      "image 69/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/2.jpg: 1280x1280 2 Whiteflys, 151.1ms\n",
      "image 70/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/20.jpg: 1280x1280 26 Whiteflys, 146.0ms\n",
      "image 71/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/22.jpg: 1280x1280 15 Whiteflys, 150.3ms\n",
      "image 72/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/23.jpg: 1280x1280 51 Whiteflys, 143.5ms\n",
      "image 73/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/25.jpg: 1280x1280 15 Whiteflys, 142.8ms\n",
      "image 74/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/27.jpg: 1280x1280 14 Whiteflys, 150.1ms\n",
      "image 75/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/29.jpg: 1280x1280 9 Whiteflys, 151.1ms\n",
      "image 76/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/3.jpg: 1280x1280 23 Whiteflys, 144.3ms\n",
      "image 77/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/31.jpg: 1280x1280 25 Whiteflys, 143.5ms\n",
      "image 78/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/34.jpg: 1280x1280 9 Whiteflys, 146.5ms\n",
      "image 79/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/36.jpg: 1280x1280 17 Whiteflys, 143.2ms\n",
      "image 80/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/37.jpg: 1280x1280 2 Whiteflys, 147.2ms\n",
      "image 81/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/38.jpg: 1280x1280 5 Whiteflys, 139.6ms\n",
      "image 82/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/39.jpg: 1280x1280 1 Whitefly, 142.1ms\n",
      "image 83/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/4.jpg: 1280x1280 35 Whiteflys, 139.5ms\n",
      "image 84/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/40.jpg: 1280x1280 10 Whiteflys, 140.5ms\n",
      "image 85/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/42.jpg: 1280x1280 16 Whiteflys, 144.0ms\n",
      "image 86/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/43.jpg: 1280x1280 37 Whiteflys, 145.0ms\n",
      "image 87/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/44.jpg: 1280x1280 6 Whiteflys, 155.6ms\n",
      "image 88/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/45.jpg: 1280x1280 6 Whiteflys, 147.5ms\n",
      "image 89/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/46.jpg: 1280x1280 9 Whiteflys, 144.9ms\n",
      "image 90/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/5.jpg: 1280x1280 8 Whiteflys, 142.9ms\n",
      "image 91/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/50.jpg: 1280x1280 10 Whiteflys, 140.6ms\n",
      "image 92/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/52.jpg: 1280x1280 15 Whiteflys, 141.8ms\n",
      "image 93/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/53.jpg: 1280x1280 5 Whiteflys, 140.5ms\n",
      "image 94/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/57.jpg: 1280x1280 15 Whiteflys, 139.6ms\n",
      "image 95/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/58.jpg: 1280x1280 26 Whiteflys, 145.4ms\n",
      "image 96/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/66.jpg: 1280x1280 20 Whiteflys, 140.0ms\n",
      "image 97/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/68.jpg: 1280x1280 30 Whiteflys, 141.7ms\n",
      "image 98/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/69.jpg: 1280x1280 30 Whiteflys, 141.6ms\n",
      "image 99/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/7.jpg: 1280x1280 25 Whiteflys, 144.7ms\n",
      "image 100/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/70.jpg: 1280x1280 12 Whiteflys, 142.3ms\n",
      "image 101/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/72.jpg: 1280x1280 3 Whiteflys, 140.6ms\n",
      "image 102/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/73.jpg: 1280x1280 13 Whiteflys, 140.7ms\n",
      "image 103/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/78.jpg: 1280x1280 9 Whiteflys, 143.2ms\n",
      "image 104/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/79.jpg: 1280x1280 49 Whiteflys, 141.9ms\n",
      "image 105/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/8.jpg: 1280x1280 25 Whiteflys, 140.3ms\n",
      "image 106/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/80.jpg: 1280x1280 6 Whiteflys, 141.0ms\n",
      "image 107/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/82.jpg: 1280x1280 16 Whiteflys, 142.4ms\n",
      "image 108/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/84.jpg: 1280x1280 5 Whiteflys, 141.9ms\n",
      "image 109/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/85.jpg: 1280x1280 4 Whiteflys, 141.6ms\n",
      "image 110/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/86.jpg: 1280x1280 8 Whiteflys, 143.2ms\n",
      "image 111/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/9.jpg: 1280x1280 23 Whiteflys, 143.7ms\n",
      "image 112/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/91.jpg: 1280x1280 6 Whiteflys, 145.7ms\n",
      "image 113/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/92.jpg: 1280x1280 4 Whiteflys, 145.5ms\n",
      "image 114/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/93.jpg: 1280x1280 6 Whiteflys, 146.2ms\n",
      "image 115/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/94.jpg: 1280x1280 11 Whiteflys, 143.9ms\n",
      "image 116/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/95.jpg: 1280x1280 24 Whiteflys, 143.4ms\n",
      "image 117/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/96.jpg: 1280x1280 13 Whiteflys, 143.2ms\n",
      "image 118/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/97.jpg: 1280x1280 9 Whiteflys, 143.6ms\n",
      "image 119/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/98.jpg: 1280x1280 13 Whiteflys, 138.8ms\n",
      "image 120/120 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/99.jpg: 1280x1280 11 Whiteflys, 142.4ms\n",
      "Speed: 6.7ms preprocess, 145.5ms inference, 0.9ms postprocess per image at shape (1, 3, 1280, 1280)\n",
      "Results saved to \u001b[1mexample_active_learning_al/detecting/al_batch_2\u001b[0m\n",
      "120 labels saved to example_active_learning_al/detecting/al_batch_2/labels\n",
      "Available indices for new batch: 120\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1280, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=al_batch_2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=example_active_learning_al/training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=example_active_learning_al/training/al_batch_2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2876.2±3881.0 MB/s, size: 743.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train... 45 images, 2 backgrounds, 0 corrupt: 100%|██████████| 45/45 [00:00<00:00, 1953.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 5642.1±5203.9 MB/s, size: 987.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val... 15 images, 0 backgrounds, 0 corrupt: 100%|██████████| 15/15 [00:00<00:00, 2149.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val.cache\n",
      "Plotting labels to example_active_learning_al/training/al_batch_2/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mexample_active_learning_al/training/al_batch_2\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G      2.717      4.026      1.136         89       1280: 100%|██████████| 12/12 [00:46<00:00,  3.84s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107    0.00267     0.0108    0.00311   0.000822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20         0G      2.374      3.423      1.012         28       1280: 100%|██████████| 12/12 [00:38<00:00,  3.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107    0.00778     0.0316    0.00514    0.00165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20         0G      2.149      7.034     0.9058          0       1280: 100%|██████████| 12/12 [00:43<00:00,  3.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107     0.0164     0.0668     0.0217     0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20         0G      2.206      2.382     0.9522         14       1280: 100%|██████████| 12/12 [00:41<00:00,  3.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:05<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107    0.00978     0.0397    0.00902    0.00209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20         0G      2.322      2.378     0.9583         95       1280: 100%|██████████| 12/12 [00:41<00:00,  3.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:06<00:00,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.006     0.0244    0.00348    0.00111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20         0G      2.266      2.348     0.9822         44       1280: 100%|██████████| 12/12 [00:41<00:00,  3.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107     0.0664       0.27      0.118     0.0471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20         0G      2.328      2.166     0.9601        147       1280: 100%|██████████| 12/12 [00:43<00:00,  3.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107     0.0664       0.27      0.118     0.0471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20         0G      2.302      2.019     0.9522         97       1280: 100%|██████████| 12/12 [00:40<00:00,  3.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.146      0.592      0.392      0.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20         0G      2.284       2.02     0.9777        159       1280: 100%|██████████| 12/12 [00:40<00:00,  3.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.644      0.393      0.494      0.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20         0G      2.339      2.103     0.9623        304       1280: 100%|██████████| 12/12 [00:39<00:00,  3.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.563      0.499      0.492       0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20         0G      2.147          2     0.9774         45       1280: 100%|██████████| 12/12 [00:38<00:00,  3.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.563      0.499      0.492       0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20         0G      2.088      2.003     0.9792         34       1280: 100%|██████████| 12/12 [00:38<00:00,  3.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.528      0.535      0.483      0.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20         0G      2.104      2.001     0.9561         73       1280: 100%|██████████| 12/12 [00:40<00:00,  3.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.526      0.516      0.475      0.181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20         0G      1.992      1.906     0.9492         28       1280: 100%|██████████| 12/12 [00:37<00:00,  3.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107       0.57      0.526       0.51      0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20         0G      2.029       1.97     0.9468        234       1280: 100%|██████████| 12/12 [00:40<00:00,  3.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107       0.57      0.526       0.51      0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20         0G      2.116      1.877     0.9573         90       1280: 100%|██████████| 12/12 [00:39<00:00,  3.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.589      0.541      0.541      0.241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20         0G       2.17       1.98     0.9717         39       1280: 100%|██████████| 12/12 [00:40<00:00,  3.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.617       0.59      0.604       0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20         0G      1.914      1.685     0.9462         14       1280: 100%|██████████| 12/12 [00:39<00:00,  3.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.602      0.595      0.597      0.281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20         0G      1.927      1.697     0.9273         28       1280: 100%|██████████| 12/12 [00:38<00:00,  3.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.602      0.595      0.597      0.281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20         0G      1.923      1.768     0.9447         17       1280: 100%|██████████| 12/12 [00:40<00:00,  3.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.605      0.612        0.6       0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.270 hours.\n",
      "Optimizer stripped from example_active_learning_al/training/al_batch_2/weights/last.pt, 5.6MB\n",
      "Optimizer stripped from example_active_learning_al/training/al_batch_2/weights/best.pt, 5.6MB\n",
      "\n",
      "Validating example_active_learning_al/training/al_batch_2/weights/best.pt...\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         15       1107      0.603      0.595      0.597      0.281\n",
      "Speed: 7.6ms preprocess, 172.7ms inference, 0.0ms loss, 286.3ms postprocess per image\n",
      "Results saved to \u001b[1mexample_active_learning_al/training/al_batch_2\u001b[0m\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 995.4±556.8 MB/s, size: 769.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/test.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|██████████| 40/40 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.800s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  33%|███▎      | 1/3 [00:11<00:23, 11.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.800s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:27<00:00,  9.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40       2060      0.674      0.525      0.578       0.33\n",
      "Speed: 3.5ms preprocess, 448.5ms inference, 0.0ms loss, 181.5ms postprocess per image\n",
      "Results saved to \u001b[1mexample_active_learning_al/testing/test\u001b[0m\n",
      "✅ Batch 3 (al) done — mAP@50: 0.5973\n",
      "\n",
      "=== Batch 3 / 8 — Mode: al ===\n",
      "Creating new batch with 15 train images and 5 validation images.\n",
      "Using active learning mode: confidence\n",
      "Available indices for new batch: 100\n",
      "Detection directory created at: dataset/detect\n",
      "\n",
      "image 1/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/0.jpg: 1280x1280 46 Whiteflys, 172.9ms\n",
      "image 2/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/1.jpg: 1280x1280 52 Whiteflys, 171.1ms\n",
      "image 3/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/101.jpg: 1280x1280 34 Whiteflys, 168.1ms\n",
      "image 4/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/102.jpg: 1280x1280 27 Whiteflys, 170.9ms\n",
      "image 5/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/104.jpg: 1280x1280 65 Whiteflys, 167.1ms\n",
      "image 6/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/106.jpg: 1280x1280 63 Whiteflys, 166.9ms\n",
      "image 7/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/107.jpg: 1280x1280 58 Whiteflys, 166.4ms\n",
      "image 8/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/108.jpg: 1280x1280 53 Whiteflys, 190.2ms\n",
      "image 9/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/11.jpg: 1280x1280 76 Whiteflys, 173.2ms\n",
      "image 10/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/110.jpg: 1280x1280 57 Whiteflys, 161.4ms\n",
      "image 11/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/111.jpg: 1280x1280 92 Whiteflys, 146.0ms\n",
      "image 12/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/113.jpg: 1280x1280 60 Whiteflys, 144.3ms\n",
      "image 13/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/114.jpg: 1280x1280 19 Whiteflys, 149.2ms\n",
      "image 14/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/115.jpg: 1280x1280 51 Whiteflys, 151.4ms\n",
      "image 15/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/118.jpg: 1280x1280 62 Whiteflys, 149.0ms\n",
      "image 16/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/119.jpg: 1280x1280 70 Whiteflys, 145.6ms\n",
      "image 17/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/120.jpg: 1280x1280 45 Whiteflys, 145.5ms\n",
      "image 18/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/121.jpg: 1280x1280 22 Whiteflys, 149.6ms\n",
      "image 19/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/122.jpg: 1280x1280 29 Whiteflys, 149.9ms\n",
      "image 20/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/123.jpg: 1280x1280 35 Whiteflys, 152.1ms\n",
      "image 21/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/125.jpg: 1280x1280 62 Whiteflys, 148.5ms\n",
      "image 22/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/126.jpg: 1280x1280 58 Whiteflys, 151.8ms\n",
      "image 23/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/127.jpg: 1280x1280 52 Whiteflys, 144.2ms\n",
      "image 24/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/134.jpg: 1280x1280 89 Whiteflys, 144.5ms\n",
      "image 25/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/135.jpg: 1280x1280 104 Whiteflys, 144.3ms\n",
      "image 26/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/136.jpg: 1280x1280 126 Whiteflys, 146.7ms\n",
      "image 27/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/137.jpg: 1280x1280 41 Whiteflys, 146.4ms\n",
      "image 28/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/14.jpg: 1280x1280 33 Whiteflys, 144.4ms\n",
      "image 29/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/140.jpg: 1280x1280 58 Whiteflys, 146.6ms\n",
      "image 30/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/141.jpg: 1280x1280 18 Whiteflys, 146.1ms\n",
      "image 31/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/143.jpg: 1280x1280 65 Whiteflys, 146.9ms\n",
      "image 32/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/144.jpg: 1280x1280 50 Whiteflys, 146.4ms\n",
      "image 33/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/146.jpg: 1280x1280 56 Whiteflys, 144.4ms\n",
      "image 34/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/15.jpg: 1280x1280 105 Whiteflys, 145.3ms\n",
      "image 35/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/150.jpg: 1280x1280 40 Whiteflys, 145.4ms\n",
      "image 36/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/152.jpg: 1280x1280 151 Whiteflys, 145.2ms\n",
      "image 37/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/154.jpg: 1280x1280 28 Whiteflys, 145.6ms\n",
      "image 38/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/155.jpg: 1280x1280 53 Whiteflys, 145.7ms\n",
      "image 39/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/156.jpg: 1280x1280 105 Whiteflys, 149.3ms\n",
      "image 40/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/157.jpg: 1280x1280 73 Whiteflys, 145.7ms\n",
      "image 41/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/164.jpg: 1280x1280 67 Whiteflys, 146.6ms\n",
      "image 42/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/166.jpg: 1280x1280 54 Whiteflys, 158.3ms\n",
      "image 43/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/168.jpg: 1280x1280 58 Whiteflys, 146.5ms\n",
      "image 44/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/17.jpg: 1280x1280 59 Whiteflys, 148.7ms\n",
      "image 45/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/178.jpg: 1280x1280 27 Whiteflys, 145.1ms\n",
      "image 46/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/179.jpg: 1280x1280 47 Whiteflys, 144.0ms\n",
      "image 47/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/182.jpg: 1280x1280 80 Whiteflys, 150.6ms\n",
      "image 48/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/183.jpg: 1280x1280 76 Whiteflys, 146.5ms\n",
      "image 49/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/184.jpg: 1280x1280 42 Whiteflys, 144.8ms\n",
      "image 50/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/186.jpg: 1280x1280 27 Whiteflys, 149.1ms\n",
      "image 51/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/187.jpg: 1280x1280 21 Whiteflys, 143.8ms\n",
      "image 52/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/19.jpg: 1280x1280 41 Whiteflys, 142.9ms\n",
      "image 53/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/190.jpg: 1280x1280 22 Whiteflys, 146.6ms\n",
      "image 54/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/191.jpg: 1280x1280 31 Whiteflys, 145.6ms\n",
      "image 55/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/192.jpg: 1280x1280 58 Whiteflys, 146.8ms\n",
      "image 56/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/194.jpg: 1280x1280 102 Whiteflys, 144.2ms\n",
      "image 57/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/195.jpg: 1280x1280 65 Whiteflys, 144.2ms\n",
      "image 58/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/197.jpg: 1280x1280 55 Whiteflys, 144.7ms\n",
      "image 59/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/198.jpg: 1280x1280 40 Whiteflys, 144.0ms\n",
      "image 60/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/199.jpg: 1280x1280 119 Whiteflys, 144.2ms\n",
      "image 61/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/2.jpg: 1280x1280 49 Whiteflys, 143.2ms\n",
      "image 62/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/20.jpg: 1280x1280 67 Whiteflys, 140.5ms\n",
      "image 63/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/23.jpg: 1280x1280 75 Whiteflys, 143.9ms\n",
      "image 64/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/25.jpg: 1280x1280 77 Whiteflys, 143.4ms\n",
      "image 65/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/27.jpg: 1280x1280 115 Whiteflys, 145.0ms\n",
      "image 66/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/3.jpg: 1280x1280 48 Whiteflys, 149.8ms\n",
      "image 67/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/31.jpg: 1280x1280 36 Whiteflys, 149.0ms\n",
      "image 68/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/36.jpg: 1280x1280 36 Whiteflys, 157.4ms\n",
      "image 69/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/37.jpg: 1280x1280 35 Whiteflys, 154.4ms\n",
      "image 70/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/38.jpg: 1280x1280 64 Whiteflys, 146.3ms\n",
      "image 71/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/39.jpg: 1280x1280 51 Whiteflys, 144.3ms\n",
      "image 72/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/4.jpg: 1280x1280 66 Whiteflys, 175.3ms\n",
      "image 73/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/40.jpg: 1280x1280 18 Whiteflys, 219.7ms\n",
      "image 74/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/42.jpg: 1280x1280 30 Whiteflys, 149.7ms\n",
      "image 75/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/43.jpg: 1280x1280 58 Whiteflys, 146.1ms\n",
      "image 76/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/44.jpg: 1280x1280 6 Whiteflys, 144.8ms\n",
      "image 77/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/45.jpg: 1280x1280 80 Whiteflys, 145.6ms\n",
      "image 78/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/46.jpg: 1280x1280 39 Whiteflys, 146.1ms\n",
      "image 79/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/5.jpg: 1280x1280 15 Whiteflys, 142.0ms\n",
      "image 80/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/52.jpg: 1280x1280 23 Whiteflys, 148.5ms\n",
      "image 81/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/57.jpg: 1280x1280 26 Whiteflys, 149.1ms\n",
      "image 82/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/58.jpg: 1280x1280 56 Whiteflys, 144.9ms\n",
      "image 83/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/66.jpg: 1280x1280 71 Whiteflys, 144.7ms\n",
      "image 84/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/68.jpg: 1280x1280 56 Whiteflys, 146.4ms\n",
      "image 85/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/69.jpg: 1280x1280 62 Whiteflys, 146.9ms\n",
      "image 86/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/7.jpg: 1280x1280 30 Whiteflys, 145.2ms\n",
      "image 87/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/70.jpg: 1280x1280 87 Whiteflys, 145.5ms\n",
      "image 88/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/78.jpg: 1280x1280 46 Whiteflys, 156.4ms\n",
      "image 89/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/79.jpg: 1280x1280 65 Whiteflys, 157.0ms\n",
      "image 90/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/8.jpg: 1280x1280 45 Whiteflys, 154.3ms\n",
      "image 91/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/82.jpg: 1280x1280 77 Whiteflys, 149.8ms\n",
      "image 92/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/85.jpg: 1280x1280 53 Whiteflys, 148.8ms\n",
      "image 93/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/86.jpg: 1280x1280 26 Whiteflys, 151.5ms\n",
      "image 94/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/9.jpg: 1280x1280 34 Whiteflys, 147.1ms\n",
      "image 95/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/91.jpg: 1280x1280 32 Whiteflys, 147.1ms\n",
      "image 96/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/94.jpg: 1280x1280 76 Whiteflys, 143.9ms\n",
      "image 97/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/95.jpg: 1280x1280 42 Whiteflys, 145.2ms\n",
      "image 98/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/96.jpg: 1280x1280 50 Whiteflys, 147.6ms\n",
      "image 99/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/97.jpg: 1280x1280 33 Whiteflys, 146.7ms\n",
      "image 100/100 /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/detect/98.jpg: 1280x1280 26 Whiteflys, 155.2ms\n",
      "Speed: 6.9ms preprocess, 150.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1280, 1280)\n",
      "Results saved to \u001b[1mexample_active_learning_al/detecting/al_batch_3\u001b[0m\n",
      "100 labels saved to example_active_learning_al/detecting/al_batch_3/labels\n",
      "Available indices for new batch: 100\n",
      "Ultralytics 8.3.160 🚀 Python-3.9.6 torch-2.7.1 CPU (Apple M3 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1280, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=al_batch_3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=example_active_learning_al/training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=example_active_learning_al/training/al_batch_3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1567.2±686.1 MB/s, size: 916.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train... 60 images, 2 backgrounds, 0 corrupt: 100%|██████████| 60/60 [00:00<00:00, 2192.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 3139.6±3590.0 MB/s, size: 885.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val... 20 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20/20 [00:00<00:00, 2650.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/ddcosta/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/dataset/run/val.cache\n",
      "Plotting labels to example_active_learning_al/training/al_batch_3/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mexample_active_learning_al/training/al_batch_3\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G      2.653      4.077      1.136        265       1280: 100%|██████████| 15/15 [00:56<00:00,  3.79s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:06<00:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20       1483   0.000167   0.000674   8.34e-05    2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20         0G      2.504      3.135      1.046        255       1280: 100%|██████████| 15/15 [01:00<00:00,  4.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:06<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20       1483     0.0005    0.00202    0.00026   5.42e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20         0G      2.312      2.521      1.001        282       1280:  20%|██        | 3/15 [00:14<00:56,  4.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) done — mAP@50: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmAP\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Run both modes\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mrun_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m run_training_loop(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[27], line 24\u001b[0m, in \u001b[0;36mrun_training_loop\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     11\u001b[0m     new_batch(\n\u001b[1;32m     12\u001b[0m         random\u001b[38;5;241m=\u001b[39m(mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;66;03m# Random selection if mode is \"random\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         al\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# Use confidence-based selection for active learning (other opriont: \"count\" - selected images with the most detections)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         img_size\u001b[38;5;241m=\u001b[39mimage_size\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Train the YOLO model with the selected data\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_yolo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/data.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use base weights for training (always start from scratch)\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/training\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/training/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/weights/best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Weights used to compute most relevant images in the next batch and for testing\u001b[39;00m\n\u001b[1;32m     35\u001b[0m test_results \u001b[38;5;241m=\u001b[39m test_yolo(\n\u001b[1;32m     36\u001b[0m     img\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[1;32m     37\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/data.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m     weights\u001b[38;5;241m=\u001b[39mweights, \u001b[38;5;66;03m# Weights from the model trained in this batch\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/testing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/utils/object_detection.py:45\u001b[0m, in \u001b[0;36mtrain_yolo\u001b[0;34m(img, epochs, data, name, project, weights, batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_yolo\u001b[39m(img\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1280\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/data.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_train\u001b[39m\u001b[38;5;124m'\u001b[39m, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m     44\u001b[0m     model \u001b[38;5;241m=\u001b[39m YOLO(weights)\n\u001b[0;32m---> 45\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/engine/model.py:799\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/engine/trainer.py:227\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/engine/trainer.py:406\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    405\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 406\u001b[0m     loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/nn/tasks.py:137\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/nn/tasks.py:337\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    336\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/utils/loss.py:270\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    266\u001b[0m pred_bboxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_decode(anchor_points, pred_distri)  \u001b[38;5;66;03m# xyxy, (b, h*w, 4)\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m _, target_bboxes, target_scores, fg_mask, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massigner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\u001b[39;49;00m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m target_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(target_scores\u001b[38;5;241m.\u001b[39msum(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Cls loss\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/utils/tal.py:84\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.forward\u001b[0;34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     76\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfull_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes),\n\u001b[1;32m     77\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_bboxes),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mOutOfMemoryError:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Move tensors to CPU, compute, then move back to original device\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/utils/tal.py:111\u001b[0m, in \u001b[0;36mTaskAlignedAssigner._forward\u001b[0;34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    Compute the task-aligned assignment.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m        target_gt_idx (torch.Tensor): Target ground truth indices with shape (bs, num_total_anchors).\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     mask_pos, align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pos_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     target_gt_idx, fg_mask, mask_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_highest_overlaps(mask_pos, overlaps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_max_boxes)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Assigned target\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/utils/tal.py:146\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.get_pos_mask\u001b[0;34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_pos_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Get positive mask for each ground truth box.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m        overlaps (torch.Tensor): Overlaps between predicted and ground truth boxes with shape (bs, max_num_obj, h*w).\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     mask_in_gts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_candidates_in_gts\u001b[49m\u001b[43m(\u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Get anchor_align metric, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_in_gts \u001b[38;5;241m*\u001b[39m mask_gt)\n",
      "File \u001b[0;32m~/Desktop/IbPRIA2015---Data-Efficient-Strategies-for-Object-Detection/.venv/lib/python3.9/site-packages/ultralytics/utils/tal.py:298\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.select_candidates_in_gts\u001b[0;34m(xy_centers, gt_bboxes, eps)\u001b[0m\n\u001b[1;32m    296\u001b[0m bs, n_boxes, _ \u001b[38;5;241m=\u001b[39m gt_bboxes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    297\u001b[0m lt, rb \u001b[38;5;241m=\u001b[39m gt_bboxes\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# left-top, right-bottom\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m bbox_deltas \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy_centers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxy_centers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(bs, n_boxes, n_anchors, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bbox_deltas\u001b[38;5;241m.\u001b[39mamin(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mgt_(eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define a function to run a full mode (AL or random)\n",
    "def run_training_loop(mode=\"al\"):\n",
    "\n",
    "    for batch in range(num_batches): # Loop through each batch\n",
    "        print(f\"\\n=== Batch {batch} / {num_batches} — Mode: {mode} ===\")\n",
    "        name = f\"{mode}_batch_{batch}\" # Name for the current batch run (directory)\n",
    "\n",
    "        if batch == 0: # First batch, use initial split\n",
    "            split_dataset(train_split=initial_train_split, val_split=initial_val_split)\n",
    "        else: # Subsequent batches, select new data\n",
    "            new_batch(\n",
    "                random=(mode == \"random\"), # Random selection if mode is \"random\"\n",
    "                al=\"confidence\", # Use confidence-based selection for active learning (other opriont: \"count\" - selected images with the most detections)\n",
    "                weights=weights, # Weights from the previous batch\n",
    "                train_images=int(batch_train_prop * dataset_size),\n",
    "                val_images=int(batch_val_prop * dataset_size),\n",
    "                project=f\"{project}_{mode}/detecting\",\n",
    "                name=name,\n",
    "                dataset=\"original\",\n",
    "                img_size=image_size\n",
    "            )\n",
    "        \n",
    "        # Train the YOLO model with the selected data\n",
    "        results = train_yolo(\n",
    "            img=image_size,\n",
    "            epochs=num_epochs,\n",
    "            data=\"dataset/data.yaml\",\n",
    "            weights=base_weights, # Use base weights for training (always start from scratch)\n",
    "            batch=batch_size,\n",
    "            name=name,\n",
    "            project=f\"{project}_{mode}/training\"\n",
    "        )\n",
    "\n",
    "        weights = f\"{project}_{mode}/training/{name}/weights/best.pt\" # Weights used to compute most relevant images in the next batch and for testing\n",
    "        test_results = test_yolo(\n",
    "            img=image_size,\n",
    "            data=\"dataset/data.yaml\",\n",
    "            weights=weights, # Weights from the model trained in this batch\n",
    "            project=f\"{project}_{mode}/testing\"\n",
    "        )\n",
    "        mAP_test = float(test_results.results_dict[\"metrics/mAP50(B)\"])\n",
    "        mAP = float(results.results_dict[\"metrics/mAP50(B)\"])\n",
    "        #results_df.loc[batch, f\"{mode}_mAP\"] = mAP\n",
    "        results_df.loc[batch, f\"{mode} mAP (test)\"] = mAP_test\n",
    "        print(f\"✅ Batch {batch + 1} ({mode}) done — mAP@50: {mAP:.4f}\")\n",
    "\n",
    "# Run both modes\n",
    "run_training_loop(mode=\"al\")\n",
    "run_training_loop(mode=\"random\")\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "# View combined results\n",
    "display(results_df)\n",
    "%matplotlib inline\n",
    "results_df.plot(\n",
    "    y=[\"al mAP (test)\", \"random mAP (test)\"],\n",
    "    marker='o',\n",
    "    title=\"Active Learning vs Random Selection (mAP@50)\"\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"# images\")\n",
    "plt.ylabel(\"mAP (test set)\")\n",
    "plt.xticks(ticks=results_df.index, labels=[int((i+1)*(batch_train_prop+batch_val_prop)*dataset_size) for i in results_df.index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080c338",
   "metadata": {},
   "source": [
    "## 🧹 Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8bedf",
   "metadata": {},
   "source": [
    "In object detection, **annotation quality** plays a critical role in model performance.\n",
    "\n",
    "Unlike classification tasks, object detection requires:\n",
    "- The correct **label** for each object\n",
    "- A precise **bounding box** tightly enclosing the object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c259abf",
   "metadata": {},
   "source": [
    "### Common Annotation Issues\n",
    "\n",
    "- **Incorrect Labels**: The object is mislabeled or assigned the wrong class.\n",
    "- **Poor Bounding Boxes**:\n",
    "  - Boxes do not align with the object\n",
    "  - Boxes are too loose or too tight\n",
    "  - Boxes include multiple objects\n",
    "- **Inconsistent Annotations** across similar samples\n",
    "\n",
    "Such errors often stem from:\n",
    "- **Repetitive labeling** tasks\n",
    "- **Fatigue** or insufficient training of annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e8398c",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/data_quality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9377da",
   "metadata": {},
   "source": [
    "### Annotation Quality Example: Original vs Improved Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e038459",
   "metadata": {},
   "source": [
    "As an illustration of the importance of annotation quality, this notebook includes **two versions of the same dataset**:\n",
    "\n",
    "- `dataset/original/`: Contains the original annotations\n",
    "- `dataset/improved/`: Contains refined, higher-quality annotations\n",
    "\n",
    "Both versions share the **same images**, but differ in the accuracy and consistency of their bounding boxes and labels.\n",
    "\n",
    "This allows us to analyze:\n",
    "- How labeling precision impacts training performance\n",
    "- Whether better annotations lead to **higher mAP** and more reliable predictions\n",
    "\n",
    "By training the same model on each version, we can measure how much improvement is achieved **purely through better annotations** — without changing the model architecture or training strategy.\n",
    "\n",
    "> This is especially useful in real-world projects, where annotation time is expensive, and correcting label errors can be more effective than adding more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041edc76",
   "metadata": {},
   "source": [
    "#### Experimentation Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a09f2",
   "metadata": {},
   "source": [
    "In this experiment, **two models are trained using the same set of images**, but with **different annotations**.  \n",
    "This setup is designed to evaluate the impact of **annotation quality** on the performance of an object detection model.\n",
    "\n",
    "- Both models share the **same training and validation splits**, ensuring a fair and controlled comparison.\n",
    "- The only difference lies in the annotations:\n",
    "  - One model uses the **original** annotation set.\n",
    "  - The other uses a set of **improved** annotations.\n",
    "\n",
    "---\n",
    "\n",
    "At the end of training, we compare the models using the **mAP@50 metric** to assess how annotation quality affects detection performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_split, split_dataset, train_yolo, test_yolo\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Get train/val split from dataset (test already fixed)\n",
    "train_split, val_split = get_split(\n",
    "    train_size=0.6,\n",
    "    val_size=0.2,\n",
    "    dataset_size=200\n",
    ")\n",
    "\n",
    "image_size = 1280 # Image size for training\n",
    "num_epochs = 20 # Number of epochs for training\n",
    "batch_size = 4 # Batch size for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef67c7c",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286bc7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_yolo(name):\n",
    "    # Train and test YOLO model with original annotations\n",
    "    train_yolo(\n",
    "        img=image_size,\n",
    "        epochs=num_epochs,\n",
    "        data=\"dataset/data.yaml\",\n",
    "        weights=\"yolo11n.pt\",\n",
    "        batch=batch_size,\n",
    "        name=name,\n",
    "        project=\"example_data_quality/train\"\n",
    "    )\n",
    "    \n",
    "    map_results = test_yolo(\n",
    "        img=image_size,\n",
    "        data=\"dataset/data.yaml\",\n",
    "        weights=f\"example_data_quality/train/{name}/weights/best.pt\",\n",
    "        project=\"example_data_quality/test\"\n",
    "    )\n",
    "    \n",
    "    return float(map_results.results_dict[\"metrics/mAP50(B)\"])\n",
    "\n",
    "#---------- Original Annotations ----------\n",
    "\n",
    "# Preparing the dataset with the original annotaions\n",
    "split_dataset(train_split=train_split, val_split=val_split, dataset=\"original\") \n",
    "# Train and test the model with original annotations\n",
    "map_original = train_and_test_yolo(\"original\")\n",
    "\n",
    "#---------- Improved Annotations ----------\n",
    "\n",
    "# Preparing the dataset with the improved annotaions\n",
    "split_dataset(train_split=train_split, val_split=val_split, dataset=\"improved\") # Preparing the dataset with the improved annotaions\n",
    "# Train and test the model with improved annotations\n",
    "map_improved = train_and_test_yolo(\"improved\") # Train and test the model with improved annotations\n",
    "\n",
    "#--------- Calculate improvement ---------\n",
    "\n",
    "# Calculate the absolute and relative improvement in mAP\n",
    "improvement = map_improved - map_original\n",
    "# Calculate the percentage improvement\n",
    "percent_improvement = (improvement / map_original) * 100 if map_original > 0 else float('inf')  # Avoid division by zero\n",
    "# Clear training outputs\n",
    "clear_output(wait=True)\n",
    "# Display the results in a Markdown format\n",
    "display(Markdown(f\"\"\"\n",
    "---\n",
    "### **Annotation Quality Comparison**\n",
    "\n",
    "- <span style=\"color:crimson\"><b>mAP with original annotations:</b></span> <code>{map_original:.4f}</code>\n",
    "- <span style=\"color:seagreen\"><b>mAP with improved annotations:</b></span> <code>{map_improved:.4f}</code>\n",
    "\n",
    "**Absolute improvement:** <b style=\"color:royalblue\">{improvement:.4f} mAP@50</b>  \n",
    "**Relative improvement:** <b style=\"color:royalblue\">{percent_improvement:.2f}%</b>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e37783",
   "metadata": {},
   "source": [
    "## 🧠 Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d691bca",
   "metadata": {},
   "source": [
    "Training object detection models from scratch can be time-consuming and data-hungry. To overcome this, we use **Transfer Learning (TL)** — a technique that leverages a model pre-trained on a large dataset (e.g., COCO) and adapts it to our custom task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca0af0",
   "metadata": {},
   "source": [
    "#### What Can Change?\n",
    "\n",
    "- The model’s **detection head** (final layers) is updated to match the number of classes in our current dataset.\n",
    "- During training, the model retains the **general visual features** it has already learned (e.g., shapes, textures), while adapting to our specific setting.\n",
    "\n",
    "> Transfer Learning helps us build accurate models faster, even with relatively small datasets — by reusing knowledge from related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce52a4d",
   "metadata": {},
   "source": [
    "\n",
    "#### Why Use Transfer Learning?\n",
    "\n",
    "- **Jump-start training** with already-learned features\n",
    "- **Reduce training time**\n",
    "- **Improve performance**, especially when labeled data is limited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28ebcd",
   "metadata": {},
   "source": [
    "### Reusing a YOLOv5 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ad421",
   "metadata": {},
   "source": [
    "#### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeed2f7",
   "metadata": {},
   "source": [
    "In this tutorial, we leverage **Transfer Learning** by starting from a pre-trained **YOLOv5** model trained to detect **whiteflies in yellow sticky traps**:\n",
    "\n",
    "`pre-trained/yellow_trap.pt`\n",
    "\n",
    "Although the original model was trained on the **same task** (whitefly detection), it was collected in a **different environment**. This makes it an excellent candidate for fine-tuning on our custom data.\n",
    "\n",
    "##### YOLOv5 pre-trained on:\n",
    "\n",
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/tl_v5.png)\n",
    "\n",
    "##### New Dataset\n",
    "\n",
    "![](https://raw.githubusercontent.com/dinisdcosta/IbPRIA2025---Data-Efficient-Strategies-for-Object-Detection/main/images/tl_new.png)\n",
    "\n",
    "As seen in the images above, both scenarios contain **whiteflies**, but in different environments.  \n",
    "A model trained on one of these image sets can be used as a **starting point** for training on the other, leveraging the benefits of **transfer learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740e666",
   "metadata": {},
   "source": [
    "> Recent YOLO models (version 8 and above) are available through the **Ultralytics Python library**.  \n",
    "> However, in this tutorial, we use a **pre-trained YOLOv5 model**, which is **not included** in the `ultralytics` package.\n",
    "\n",
    "> Therefore, we need to **clone the YOLOv5 repository manually** to access its training and inference utilities.\n",
    "\n",
    "> **Note:** The YOLOv5 repository is no longer actively maintained and may produce several warnings during training due to updates in related packages.  \n",
    "> Despite this, it remains **fully functional** and suitable for our fine-tuning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43a4ed",
   "metadata": {},
   "source": [
    "#### Cloning YOLOv5 github repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f96458",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f374e",
   "metadata": {},
   "source": [
    "#### Experience Preparation\n",
    "\n",
    "In this transfer learning experiment, we train **two distinct models** for comparison:\n",
    "\n",
    "- **Model A:** Starts training using the weights of a **pre-trained YOLOv5** model  \n",
    "- **Model B:** Trains **from scratch** using **YOLOv11n**, with no prior learning\n",
    "\n",
    "Both models are trained using the **same dataset**, including:\n",
    "\n",
    "- Identical **training and validation splits**\n",
    "- Same **number of epochs**, **image size**, and **batch size**\n",
    "\n",
    "---\n",
    "\n",
    "At the end of training, a plot is displayed showing the **evolution of performance (mAP@50)** across epochs for both models, allowing us to visualize and compare their learning behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4180ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_split, split_dataset, train_yolo, train_yolov5\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get train/val split from dataset (test already fixed 20%)\n",
    "train_split, val_split = get_split(\n",
    "    train_size=0.6, # 60 percent of the dataset for training\n",
    "    val_size=0.2, # 20 percent of the dataset for validation\n",
    "    dataset_size=200\n",
    ")\n",
    "\n",
    "# Images to be used for training and validation will be available in dataset/run\n",
    "split_dataset(train_split=train_split, val_split=val_split, dataset=\"improved\")\n",
    "\n",
    "num_epochs = 5 # Number of epochs for training\n",
    "batch_size = 4 # Batch size for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325500af",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7b96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully. Results saved in 'example_transfer_learning/train/transfer_learning' and 'example_transfer_learning/train/baseline'.\n"
     ]
    }
   ],
   "source": [
    "# Train YOLOv5 with transfer learning using pre-trained weights\n",
    "train_yolov5(\n",
    "    img=1280, # pre-trained/yellow_trap.pt was trained on 1280x1280 images, so we use the same size\n",
    "    epochs=num_epochs,\n",
    "    data=\"dataset/data_yolov5.yaml\",\n",
    "    weights=\"pre-trained/yellow_trap.pt\",  # Using YOLOv5 small as the pre-trained model\n",
    "    batch=batch_size,\n",
    "    name=\"transfer_learning\",\n",
    "    project=\"example_transfer_learning\"\n",
    ")\n",
    "\n",
    "# Train YOLOv11 from scratch on the same dataset\n",
    "train_yolo(\n",
    "    img=1280,\n",
    "    epochs=num_epochs,\n",
    "    data=\"dataset/data.yaml\",\n",
    "    weights=\"yolo11n.yaml\",\n",
    "    batch=batch_size,\n",
    "    name=\"baseline\",\n",
    "    project=\"example_transfer_learning/train\"\n",
    ")\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "print(\"Training completed successfully. Results saved in 'example_transfer_learning/train/transfer_learning' and 'example_transfer_learning/train/baseline'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e298b1",
   "metadata": {},
   "source": [
    "#### Plot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd207f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"example_transfer_learning/train/transfer_learning/results.csv\")\n",
    "df2 = pd.read_csv(\"example_transfer_learning/train/baseline/results.csv\")\n",
    "\n",
    "# Strip leading/trailing whitespace from column names\n",
    "df1.columns = df1.columns.str.strip()\n",
    "df2.columns = df2.columns.str.strip()\n",
    "\n",
    "%matplotlib inline\n",
    "# Plotting mAP@0.5\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df1['metrics/mAP_0.5'], label='Transfer Learning', marker='o')\n",
    "plt.plot(df2['metrics/mAP50(B)'], label='Baseline', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('mAP@0.5')\n",
    "plt.title('Comparison of mAP@0.5 Across Experiments')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
